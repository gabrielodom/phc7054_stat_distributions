---
title: "The Normal Distribution"
author: "Gabriel J. Odom"
date: last-modified
format:
  html:
    embed-resources: false
    code-fold: true
knitr:
  opts_chunk:        ########## set global options ############
    collapse: true   # keep code from blocks together (if shown)
    echo: true       # show code
    message: true    # show messages
    warning: true    # show warnings
    error: true      # show error messages
    comment: ""      # don't show ## with printed output
    dpi: 100         # image resolution (typically 300 for publication)
    fig-width: 6.5   # figure width
    fig-height: 4.0  # figure height
    R.options:    
      digits: 3    # round to three digits
editor: source
---

```{r, tidyverse-tidymodels}
#| echo: false

library(conflicted)
suppressPackageStartupMessages(library(tidymodels))
tidymodels_prefer()
suppressPackageStartupMessages(library(tidyverse))

# suppress "`summarise()` has grouped output by " messages
options(dplyr.summarise.inform = FALSE)
```



## Deriving the Error Distribution
The Normal Distribution (also called the Gaussian Distribution after **Carl Friedrich Gauss**^[<https://wikipedia.org/wiki/Carl_Friedrich_Gauss>]) is first and foremost the "mistakes" distribution. Gauss derived this distribution to explain the discrepencies between multiple junior (or even senior) astronomers' reported planetary positions. As a quick note, *this derivation is absolutely monstrous* (Gauss' genius was orders of magnitude beyond my intelligence). We will follow the general premise of Prof. Xi Chen's translation and understanding of Gauss' original notes (and Prof. Chen's comprehensive understanding of the history of mathematics and statistics); he published his thoughts in his blog "Not a Rocket Scientist" in [his 27 January 2023 post](https://notarocketscientist.xyz/posts/2023-01-27-how-gauss-derived-the-normal-distribution/).


### Preliminary Notation and Assumptions
We begin by noting that if we have only one measurement, that single measurement is our best guess for the truth. Let's denote it $m$. Also, we assume that:

1. If we add more independent measurements, we can use the arithmetic mean to "summarize" them well.
2. If we add more independent measurements from similarly trained astronomers, these new values can be described positive or negative deviations (i.e., mistakes or errors) from the first measurement.
3. These mistakes/deviations/errors should be independently and symmetrically distributed around 0
    - by random chance, an astronomer is just as likely to write down a number slightly smaller than the "truth" as they would be to write down a slightly larger number; and,
    - these astronomers are all similarly trained and trying their best, so we expect the mistake distribution to be the same for all astronomers.
4. Smaller mistakes/deviations/errors are more likely to occur than larger errors.

> "It has been customary to regard as an axiom the hypothesis that if any quantity has been determined by several direct observations, made under the same circumstances and with equal care, the arithmetic mean of the observed values will yield the most probable value...
>
> Gauss, *Theoria Motus*, as shown in "Gauss' First Argument for Least Squares", p. 43. <https://www.jstor.org/stable/41133877?seq=3>

We start by assuming that the first measurement is the correct one (which is obviously wrong, but it allows us to understand what's happening, and the assumption will be quickly discarded). If we have $n$ independent attempts to measure the same true value (like the angle of one planet relative to another), denoted $X_1, X_2, \ldots, X_n$, the mistake (error) components of these measures can be defined as
$$
[E_1, E_2, \ldots, E_n] = [X_1, X_2, \ldots, X_n] - m.
$$


### Applying Assumptions
As we assumed above, these mistakes are all independent and identically distributed according to some distribution. We denote this as
$$
E_i \overset{iid}{\sim} f_E(\varepsilon).
$$
As we already assumed, errors are symmetric around 0. Thus, we know that $f_E(\varepsilon) = f_E(-\varepsilon)$. Further, because we also assume that errors near 0 are **more likely** than errors further away, this leads us to create a likelihood function. While, we do have a vector of observed data (the errors, $E_i$), we don't know what kinds of parameter space we are optimizing over. So, we will present the general form of some likelihood function optimization with an unspecified parameter suite (the empty dot):
$$
\begin{aligned}
\mathcal{L}(\circ|\textbf{E}) &= \prod_{i = 1}^n f(\varepsilon_i|\circ) \\
\Longrightarrow \ell(\circ|\textbf{E}) &= \sum_{i = 1}^n \log\left[ f(\varepsilon_i|\circ) \right] \\
\Longrightarrow \frac{\partial}{\partial\circ} \ell(\circ|\textbf{E}) &= \sum_{i = 1}^n \frac{f^{\prime}(\varepsilon_i|\circ)}{f(\varepsilon_i|\circ)}.
\end{aligned}
$$

Now we have to take a step back here. We have no idea what $f$ looks like, so we have no idea what $\ell$ looks like, and we *certainly* have no idea to take a derivative with respect to some unknown group of parameters (we don't even know how many there are), or even if we are taking the derivative with respect to the parameters at all! All we know is that we are taking the derivative of some function $\ell$ with respect to something, and that we are treating the data as fixed so that we can find the functions $\ell$ and $f$ which fit the data best.

To make the notation a touch easier, let $g_E(\varepsilon) = f^{\prime}(\varepsilon_i) / f(\varepsilon_i)$. Now, let's apply some of Gauss' intuition, that the arithmetic mean would be a good approximation of the truth; that is, Gauss assumed that $\bar{X} \approx m$. Thus,
$$
\begin{aligned}
\frac{\partial}{\partial\circ} \ell(\circ|\textbf{E}) &= \sum_{i = 1}^n \frac{f^{\prime}(\varepsilon_i|\circ)}{f(\varepsilon_i|\circ)} \\
&\qquad\text{\emph{Easier notation...}} \\
&= \sum_{i = 1}^n g_E(\varepsilon) \\
&\qquad\text{\emph{By definition...}} \\
&= \sum_{i = 1}^n g_E(X_i - m) \\
&\qquad\text{\emph{Gauss' intuition...}} \\
&\approx \sum_{i = 1}^n g_E(X_i - \bar{X}) \\
&\qquad\text{\emph{Assumption that mistakes centre around 0...}} \\
&= 0.
\end{aligned}
$$
This may look trivial (obviously the sum of mistakes which centre at 0 should be approximately 0), but it allows us to make more clarifying statements about the original error function. Now, we have that we are looking for some $f(\textbf{E}|\boldsymbol\theta) \ni \nabla\ell(\boldsymbol\theta|\textbf{E}) = 0$^[This upside down triangle is called the "Del" or "nabla" symbol, and it denotes the vector of all possible first-order partial derivatives (because we don't know how many dimensions the parameter space has). See <https://en.wikipedia.org/wiki/Del>]. Notice that this relationship will hold for any properly constructed sample $\textbf{X}$, because we are *constructing* $f$ to have this property.


### Clues About $g_E$
We are being good little detectives so far. We know a couple things now:
$$
\begin{aligned}
\nabla \ell(\boldsymbol\theta|\textbf{E}) \approx \sum_{i = 1}^n g_E(X_i - \bar{X}) &= 0 \\
\sum_{i = 1}^n x_i - n\bar{x} &= 0.
\end{aligned}
$$
The first is what we discovered last section. The second is by definition. So we need a family of functions for $g_E$ so that
$$
\sum_{i = 1}^n g_E(X_i - \bar{X}) = \sum_{i = 1}^n X_i - n\bar{X}.
$$
So, we need a function $g_E$ that will allow the summation operator to "pass through", so that 
$$
\sum_{i = 1}^n g_E(X_i - \bar{X}) = g_E\left( \sum_{i = 1}^n [X_i - \bar{X}]. \right)
$$
This means that we need $g_E$ to be a linear transformation^[It's technically a linear function, but that term has too many definitions. We need the "linear function" that means "non-affine". See <https://en.wikipedia.org/wiki/Linear_map>]. Also, we need
$$
\begin{align}
0 &= \sum_{i = 1}^n g_E(X_i - \bar{X}) \\
&\qquad\text{\emph{We just assumed this...}} \\
&= g_E\left( \sum_{i = 1}^n [X_i - \bar{X}] \right) \\
&= g_E\left( \left[\sum_{i = 1}^n X_i\right] - n\bar{X} \right) \\
&\qquad\text{\emph{Because it's a linear transformation...}} \\
&= g_E\left( \sum_{i = 1}^n X_i \right) - g_E\left( n\bar{X} \right).
\end{align}
$$
This last requirement is much more restrictive than you might think. We think of linear functions as anything of the form $y = mx + b$, but this is technically an **affine transformation**^[<https://en.wikipedia.org/wiki/Affine_transformation>]. Our function $g_E$, in order to have the properties we need, cannot have a "shift" term. Thus, for some unknown constant $C$,
$$
g_E(\varepsilon) = C\varepsilon.
$$
Only a function of this form will allow 
$$
\begin{aligned}
0 &= \sum_{i = 1}^n X_i - n\bar{X} \\
&= g_E\left( \sum_{i = 1}^n X_i \right) - g_E\left( n\bar{X} \right) \\
&= g_E\left( \sum_{i = 1}^n [X_i - \bar{X}] \right) \\
&= \sum_{i = 1}^n g_E(X_i - \bar{X})
\end{aligned}
$$
to all be simultaneously true.


### A Form for $\ell$
Finally, these baby steps are starting to add up! We now know that
$$
\sum_{i = 1}^n g_E(\varepsilon_i|\boldsymbol\theta) = \sum_{i = 1}^n C\varepsilon_i.
$$
This is incredible! We now can see that there is only one parameter in the derivative of our unknown parameter space, and it is some constant $C$.

This allows us to "bend" our thinking again. When we started, we had to treat the data as fixed, and the form of $f$ and $\ell$ as unknown. Well, now we have a form for the derivative of the log of $f$. So, we will pivot, and now treat the form of $f$ as fixed, and treat the data as unknown. This allows us to integrate with respect to the changing data, $\varepsilon$. Thus, for a single unknown data point, we have a differential equation:
$$
\begin{aligned}
C\varepsilon &= g_E(\varepsilon|C) \\
&= \frac{\frac{d}{d\varepsilon} f_E(\varepsilon|C)}{f_E(\varepsilon|C)} \\
\Longrightarrow \int C\varepsilon d\varepsilon &= \int \frac{\frac{d}{d\varepsilon} f_E(\varepsilon|C)}{f_E(\varepsilon|C)} d\varepsilon \\
\Longrightarrow \frac{C_1}{2}\varepsilon^2 + C_2 &= \log\left[ f_E(\varepsilon|C) \right] + C_3 \\
\Longrightarrow \frac{C_1}{2}\varepsilon^2 + (C_2 - C_3) &= \log\left[ f_E(\varepsilon|C) \right] \\
\Longrightarrow e^{\frac{C_1}{2}\varepsilon^2 + (C_2 - C_3)} &= f_E(\varepsilon|C).
\end{aligned}
$$

Notice that this form of $f_E$ has three unknown constants, so let's clean these up:
$$
f_E(\varepsilon|C) = e^{\frac{C_1}{2}\varepsilon^2 + (C_2 - C_3)} = Ae^{\frac{C_1}{2}\varepsilon^2}.
$$

Now, let's check our original assumptions about $f_E(\varepsilon)$.

1. We assumed that $f_E(\varepsilon) = f_E(-\varepsilon)$. Because we have an $e^{h\varepsilon^2}$, then negative or positive errors of the same magnitude will appear with the same likelihood^[Notice I said likelihood, because we are still dealing with the form of $\ell$, and we haven't gotten to any probabilities yet.].
2. We assumed that larger errors are less likely than smaller errors. Well, this assumption is only true for $C_1 < 0$, so let's make this explicit and build a negative sign directly into $f_E$. So,
$$
f_E(\varepsilon|C) = Ae^{-\frac{C}{2}\varepsilon^2}.
$$

Finally, we see that our likelihood function, $\ell$, will be built from this $f$ which has these two properties: 1) that errors are symmetric around 0, and 2) that smaller errors are more likely than larger errors.


### Integrating the Likelihood to Find a Distribution
As we can see in the form of $f_E$ above, this function will always be non-negative. So, as we've done before, to transition from likelihood to a probability function, we will **marginalize** this function to find the values of $A$ and $C$ which ensure that the total probability for this error distribution is equal to 1. But, because we have only one equation and two unknowns, we will solve for $A$ as a function of $C$ That is, we need to solve for $A_C$ so that
$$
\int_{\mathcal{S}(\varepsilon)} dF(\varepsilon|C) = \int_{-\infty}^{\infty} f_E(\varepsilon|C) d\varepsilon = \int_{-\infty}^{\infty} A_Ce^{-\frac{C}{2}\varepsilon^2} d\varepsilon =  1.
$$

This integral will require quite a few substeps and substitutions to solve. We are solving for $A$ as a function of $C$ that will allow this integral to converge to 1. Let's begin by changing this single integral to a double integral, and then modifying the bounds of the integration.
$$
\begin{aligned}
1 &= \int_{-\infty}^{\infty} A_Ce^{-\frac{C}{2}\varepsilon^2} d\varepsilon \\
\Longrightarrow 1^2 &= \left[ \int_{-\infty}^{\infty} A_Ce^{-\frac{C}{2}\varepsilon^2} d\varepsilon \right] \times \left[ \int_{-\infty}^{\infty} A_Ce^{-\frac{C}{2}\varepsilon^2} d\varepsilon \right] \\
&= \left[ \int_{-\infty}^{\infty} A_Ce^{-\frac{C}{2}\varepsilon^2} d\varepsilon \right] \times \left[ \int_{-\infty}^{\infty} A_Ce^{-\frac{C}{2}\varphi^2} d\varphi \right] \\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} A^2_C e^{-\frac{C}{2}\varepsilon^2} e^{-\frac{C}{2}\varphi^2} d\varepsilon d\varphi \\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} A^2_C e^{-\frac{C}{2} \left( \varepsilon^2 + \varphi^2 \right)} d\varepsilon d\varphi \\
&\qquad\text{\emph{Recall that }} f_E \text{\emph{ is symmetric around 0...}} \\
&= \int_{\varphi = 0}^{\infty} 2 \int_{\varepsilon = 0}^{\infty} 2A^2_C e^{-\frac{C}{2} \left( \varepsilon^2 + \varphi^2 \right)} d\varepsilon d\varphi \\
&= 4A^2_C \int_{\varphi = 0}^{\infty} \int_{\varepsilon = 0}^{\infty} e^{-\frac{C}{2} \left( \varepsilon^2 + \varphi^2 \right)} d\varepsilon d\varphi.
\end{aligned}
$$

Now, we will make our first substitution. Let $\varphi = t\varepsilon$, so $d\varphi = \varepsilon dt$. Because this is a linear function, the bounds of integration for $t$ will be equivalent to those for $\varphi$.
$$
\begin{aligned}
1^2 &= 4A^2_C \int_{\varphi = 0}^{\infty} \int_{\varepsilon = 0}^{\infty} e^{-\frac{C}{2} \left( \varepsilon^2 + \varphi^2 \right)} d\varepsilon d\varphi \\
&= 4A^2_C \int_{[t] = 0}^{\infty} \int_{\varepsilon = 0}^{\infty} e^{-\frac{C}{2} \left( \varepsilon^2 + [t\varepsilon]^2 \right)} d\varepsilon [\varepsilon dt] \\
&= 4A^2_C \int_{t = 0}^{\infty} \int_{\varepsilon = 0}^{\infty} e^{-\frac{C}{2} \varepsilon^2 (1 + t^2)} \varepsilon d\varepsilon dt.
\end{aligned}
$$

It took a bit of work, but we finally have an integrand that looks like $ve^{v^2}dv$, which means that we can use $u$-substitution. Hence, we let
$$ 
u = -\frac{C}{2} \varepsilon^2 (1 + t^2) \Rightarrow du = -C\varepsilon (1 + t^2) d\varepsilon \Rightarrow -\frac{du}{C(1 + t^2)} = \varepsilon d\varepsilon.
$$
Our bounds of integration do change for this transformation: when $\varepsilon = 0, u = 0$, but when $\varepsilon \to \infty, u \to -\infty$. For these steps, we will also flip the bounds of integration, and, at the last integral step, recognize the derivative of the arctangent function (which we covered in the Formal Foundations chapter with the review section on trigonometry):
$$
\begin{aligned}
1^2 &= 4A^2_C \int_{t = 0}^{\infty} \int_{\varepsilon = 0}^{\infty} e^{-\frac{C}{2} \varepsilon^2 (1 + t^2)} \varepsilon d\varepsilon dt \\
&= 4A^2_C \int_{t = 0}^{\infty} \int_{[u] = 0}^{-\infty} e^{[u]} \left[ -\frac{du}{C(1 + t^2)} \right] dt \\
&\qquad\text{\emph{Flip bounds of integration...}} \\
&= 4A^2_C \int_{t = 0}^{\infty} \int_{u = -\infty}^0 -e^{u} \left[ -\frac{du}{C(1 + t^2)} \right] dt \\
&\qquad\text{\emph{Separable integrals...}} \\
&= 4A^2_C \int_{t = 0}^{\infty} \left[ \frac{1}{C(1 + t^2)} \right] \left( \int_{u = -\infty}^0 e^{u} du \right) dt \\
&= 4A^2_C \int_{t = 0}^{\infty} \left[ \frac{1}{C(1 + t^2)} \right] \left( \lim_{k\to -\infty} \left[ e^{u} \right]_{u = k}^0 \right) dt \\
&= 4A^2_C \int_{t = 0}^{\infty} \left[ \frac{1}{C(1 + t^2)} \right] \left[ e^{[0]} - \lim_{k\to -\infty} e^{[k]} \right] dt \\
&\qquad\text{\emph{Recognize trigonometric derivative...}} \\
&= \frac{4}{C} A^2_C \int_{t = 0}^{\infty} \left[ \frac{1}{1 + t^2} \right] [1] dt \\
&= \frac{4}{C} A^2_C \left[ \lim_{k\to\infty} \arctan(t) \right]_0^k \\
&= \frac{4}{C} A^2_C \left[ \lim_{k\to\infty} \arctan(k) - \arctan(0) \right] \\
&\qquad\text{\emph{Horizontal asymptote...}} \\
&= \frac{4}{C} A^2_C \left[ \frac{\pi}{2} - 0 \right] \\
\Longrightarrow 1 &= \frac{2\pi}{C} A^2_C \\
&\qquad\text{\emph{Solve for }} A_C \\
\Longrightarrow \frac{C}{2\pi} &= A^2_C \\
\Longrightarrow \sqrt{\frac{C}{2\pi}} &= A_C.
\end{aligned}
$$

Therefore, the normalizing constant to make $\ell$ a probability function (in terms of some parameter $C$) is $\sqrt{C/(2\pi)}$. Thus, the error distribution is
$$
f_E(\varepsilon|C) = \sqrt{\frac{C}{2\pi}} e^{-\frac{C}{2}\varepsilon^2}.
$$

</br>



## Deriving the Normal Distribution from the Error Distribution

</br>



## Example Random Samples

```{r, random-sample}
set.seed(20150516)

xStandard <- rnorm(n = 1000, mean = 0, sd = 1)
samplesStd_ls <- list(
  n5    = xStandard[1:5],
  n30   = xStandard[1:30],
  n60   = xStandard[1:60],
  n1000 = xStandard
)

xShift <- rnorm(n = 1000, mean = 1, sd = 2)
samplesShifted_ls <- list(
  n5    = xShift[1:5],
  n30   = xShift[1:30],
  n60   = xShift[1:60],
  n1000 = xShift
)

range_num <- range(c(xStandard, xShift))

rm(xSymm, xSkew)
```

```{r}
#| label: shared-density-plotting-function
PlotSharedDensity <- function(x, range_x, bandwidth = "nrd0") {
  
  xDens_ls <- density(x, bw = bandwidth)
  xHist_ls <- hist(x, plot = FALSE)
  yLargest_num <- max(max(xDens_ls$y), max(xHist_ls$density))
  
  hist(
    x, prob = TRUE,
    xlim = range_x, ylim = c(0, yLargest_num)
  )
  lines(xDens_ls, col = 4, lwd = 2)
  
}
```


```{r}
#| label: random-sample-hist-symm
#| fig-show: "hold"

par(mfrow = c(2, 2))

PlotSharedDensity(
  x = samplesStd_ls$n5, range_x = range_num
)
PlotSharedDensity(
  x = samplesStd_ls$n30, range_x = range_num
)
PlotSharedDensity(
  x = samplesStd_ls$n60, range_x = range_num
)
PlotSharedDensity(
  x = samplesStd_ls$n1000, range_x = range_num
)

par(mfrow = c(1, 1))

# , bandwidth = 0.005
```

```{r}
#| label: random-sample-hist-diffuse
#| fig-show: "hold"

par(mfrow = c(2, 2))

PlotSharedDensity(
  x = samplesShifted_ls$n5, range_x = range_num
)
PlotSharedDensity(
  x = samplesShifted_ls$n30, range_x = range_num
)
PlotSharedDensity(
  x = samplesShifted_ls$n60, range_x = range_num
)
PlotSharedDensity(
  x = samplesShifted_ls$n1000, range_x = range_num
)

par(mfrow = c(1, 1))
```


</br>



## Show that this is a Distribution


</br>



## Derive the Moment Generating Function


</br>



## Method of Moments Estimates from Observed Data
Let's generate some random data...


### $\mathbb{E}[k]$


### $\mathbb{E}[k^2]$ and $\text{Var}[k]$


### Solving the System


</br>



## Maximum Likelihood Estimators


</br>



## Exercises

To be determined.


## Footnotes 


