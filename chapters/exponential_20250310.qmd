---
title: "The Exponential Distribution"
author: "Gabriel J. Odom"
date: last-modified
format:
  html:
    embed-resources: false
    code-fold: true
knitr:
  opts_chunk:        ########## set global options ############
    collapse: true   # keep code from blocks together (if shown)
    echo: true       # show code
    message: true    # show messages
    warning: true    # show warnings
    error: true      # show error messages
    comment: ""      # don't show ## with printed output
    dpi: 100         # image resolution (typically 300 for publication)
    fig-width: 6.5   # figure width
    fig-height: 4.0  # figure height
    R.options:    
      digits: 3    # round to three digits
editor: source
---

```{r, tidyverse-tidymodels}
#| echo: false

library(conflicted)
suppressPackageStartupMessages(library(tidymodels))
tidymodels_prefer()
suppressPackageStartupMessages(library(tidyverse))

# suppress "`summarise()` has grouped output by " messages
options(dplyr.summarise.inform = FALSE)
```

## Background

The **Exponential distribution** models a **waiting time** until the first event in a Poisson process with constant rate $\lambda>0$. If events occur independently and at a constant average rate, then the time $T$ until the next event has a density
$[f_{\text{Exp}}(t\mid \lambda)=\lambda e^{-\lambda t},\quad t\ge 0,]$
with CDF $F(t)=1-e^{-\lambda t}$, mean $\mathbb E[T]=1/\lambda$, and variance $\operatorname{Var}(T)=1/\lambda^2$.

**Key properties**
- **Memoryless:** $\Pr(T>s+t\mid T>s)=\Pr(T>t)$. The future doesn’t depend on how long you’ve already waited.
- **Poisson link:** If $N(t)\sim\text{Pois}(\lambda t)$ counts events in time $t$, then the **inter-arrival times** are i.i.d. Exponential($\lambda$). Conversely, summing $k$ i.i.d. Exponential($\lambda$) gives a **Gamma**($k,\lambda$) waiting time for the $k$-th event.
- **Discrete analogue:** The Exponential mirrors the **Geometric** distribution, which is memoryless on the integers.

These features make the Exponential a natural baseline for **time-to-event** phenomena where the **hazard is constant**.

### Case Study: Inter-arrival Times in Public Health Operations

Consider a **poison control hotline** (or an ED triage desk) where calls (or arrivals) can be treated as a Poisson process over short stationary periods (e.g., overnight hours). If the average **arrival rate** is $\lambda=12$ per hour (one every 5 minutes), then the **waiting time** $T$ between consecutive arrivals should be Exponential($\lambda$):
- $\Pr(T>10\text{ min})=\exp[-\lambda(10/60)]\approx e^{-2}\approx 0.135$,
- median wait $\log(2)/\lambda\approx 3.47$ minutes,
- and the **memoryless** check: $\Pr(T>5+3\mid T>5)=\Pr(T>3)$.

Operationally, this model helps staffing: a constant hazard implies predictable tail risks for long idle periods and for short bursts of back-to-back arrivals (via the Poisson–Exponential connection).

### Current Uses in Biostatistics and Public Health

- **Survival/Time-to-event modeling:** As a **baseline constant-hazard** model; often a starting point before moving to Weibull, Gompertz, or Cox PH when hazards vary.
- **Health services & operations:** Inter-arrival times for **ED arrivals**, EMS calls, lab submissions, or imaging requests; queueing and capacity planning.
- **Infectious disease control:** Time between events (e.g., calls, test-positives) during stable phases of surveillance.
- **Reliability/biomedical devices:** Time to failure for components with approximately constant failure rate (e.g., pumps, sensors) over a phase of their life cycle.

When the hazard is not constant (e.g., learning curves, wear-out), analysts typically move beyond Exponential to more flexible hazard shapes.


## Deriving the Distribution
Let's consider counting events happening in an observation interval with time $T \in [0,t)$. The count of these events are generated according to a Poisson process with rate $\lambda$ and observation interval width $t$, given by
$$
f_{\text{Pois}}(k|\lambda t) \equiv \frac{(\lambda t)^k}{k!} e^{-\lambda t}.
$$
We can continue our "clinic" example from the Poisson lesson; a Poisson process could be used to describe the number of patients who visit a clinic in a one-hour period. Let's say that we opened up the clinic in the morning, and we ask ourselves "how long do we have to wait until the first patient arrives?" Mathematically, this would be asking what the value of $T$ will be when $k$ increments from 0 to 1.

The probability that we have not yet observed an event by time $t$ is given by
$$
\mathbb{P}(0|\lambda, T = t) = \frac{(\lambda t)^0}{0!} e^{-\lambda t} = e^{-\lambda t}.
$$
Notice that we can modify our question from asking about a random variable $k$ to asking about a random variable $T$. We then see that these two quantities are the same, but their random variables are different:
$$
\mathbb{P}_k(0|T = t, \lambda) = \mathbb{P}_T(T > t|\lambda).
$$
In vernacular, we are saying that the probability of observing a *count* of zero events on or before time $t$ is the same as the probability that the *time* of the first event is after (greater than) $t$.

Because of the logical equivalence of these events, we can say that the probability that the first event (which happens at time $T$) hasn't happened yet (as of time $t$) is given by
$$
\mathbb{P}(T > t) = e^{-\lambda t}.
$$
This is the probability that the first event will happen *after* time $t$. Thus, the probability that the first event will happen *before* the end of our observation interval (which ends at time $t$) will be the opposite of this:
$$
\mathbb{P}(T \le t) = 1 - e^{-\lambda t}.
$$

Notice that we have just described the **cumulative probability function**^[<https://en.wikipedia.org/wiki/Cumulative_distribution_function>]. As we all remember, the Cumulative Probability Function is the indefinite integral of the Probability Function, so to find a closed form of the Exponential distribution, we take the derivative with respect to the random variable $t$. So, for $\lambda,t > 0$,
$$
\begin{aligned}
F_T(t|\lambda) &= 1 - e^{-\lambda t} \\
\Longrightarrow f_T(t|\lambda) &= \frac{d}{dt} \left( 1 - e^{-\lambda t} \right) \\
&= - e^{-\lambda t} (-\lambda) \\
\Longrightarrow f_{\text{Exp}}(t|\lambda) &= \lambda e^{-\lambda t}.
\end{aligned}
$$


</br>



## Example Random Samples

```{r, random-sample}
set.seed(20150516)

xPeaked <- rexp(n = 500, rate = 5)
samplesPeaked_ls <- list(
  n5   = xPeaked[1:5],
  n50  = xPeaked[1:50],
  n100 = xPeaked[1:100],
  n500 = xPeaked
)

xDiffuse <- rexp(n = 500, rate = 1)
samplesDiffuse_ls <- list(
  n5   = xDiffuse[1:5],
  n50  = xDiffuse[1:50],
  n100 = xDiffuse[1:100],
  n500 = xDiffuse
)

range_num <- range(c(xPeaked, xDiffuse))

rm(xPeaked, xDiffuse)
```

```{r}
#| label: shared-density-plotting-function
PlotSharedDensity <- function(x, range_x, bandwidth = "nrd0") {
  
  xDens_ls <- density(x, bw = bandwidth)
  xHist_ls <- hist(x, plot = FALSE)
  yLargest_num <- max(max(xDens_ls$y), max(xHist_ls$density))
  
  hist(
    x, prob = TRUE,
    xlim = range_x, ylim = c(0, yLargest_num)
  )
  lines(xDens_ls, col = 4, lwd = 2)
  
}
```


```{r}
#| label: random-sample-hist-peaked
#| fig-show: "hold"

par(mfrow = c(2, 2))

PlotSharedDensity(
  x = samplesPeaked_ls$n5, range_x = range_num
)
PlotSharedDensity(
  x = samplesPeaked_ls$n50, range_x = range_num
)
PlotSharedDensity(
  x = samplesPeaked_ls$n100, range_x = range_num
)
PlotSharedDensity(
  x = samplesPeaked_ls$n500, range_x = range_num
)

par(mfrow = c(1, 1))

# , bandwidth = 0.005
```

```{r}
#| label: random-sample-hist-diffuse
#| fig-show: "hold"

par(mfrow = c(2, 2))

PlotSharedDensity(
  x = samplesDiffuse_ls$n5, range_x = range_num
)
PlotSharedDensity(
  x = samplesDiffuse_ls$n50, range_x = range_num
)
PlotSharedDensity(
  x = samplesDiffuse_ls$n100, range_x = range_num
)
PlotSharedDensity(
  x = samplesDiffuse_ls$n500, range_x = range_num
)

par(mfrow = c(1, 1))
```


</br>

## Example with Data
`Faithful` eruption data records 250+ observations of the Old Faithful geyser in Yellowstone National Park, Wyoming in the 1970s by the National Park Service. Each observation includes two continuous variables: `eruptions` (duration in minutes) and `waiting` (time to the next eruption in minutes). The data set is found in base R and is often used to study relationships between eruption length and waiting time, as well as to model inter-eruption waiting times as realizations from an exponential or gamma distribution.

```{r}
data("faithful")
x <- faithful$waiting
n <- length(x)
head(faithful)
summary(x)
```

## Variable Distribution

The `waiting` variable contains 272 observations representing minutes between eruptions.
@fig-faithful-dist shows a histogram of waiting along with a fitted exponential density based on the estimated rate parameter.

```{r}
#| label: fig-faithful-dist
#| fig-cap: "Histogram of waiting times with Exponential(λ̂) density"
#| warning: false
library(ggplot2)
library(patchwork)

lambda_hat <- 1 / mean(x)

p_hist <- ggplot(data.frame(x), aes(x)) +
geom_histogram(aes(y = ..density..), bins = 20, alpha = 0.6, fill = "#1f4e79") +
stat_function(fun = dexp, args = list(rate = lambda_hat), color = "black", linewidth = 1) +
labs(x = "Waiting time (min)", y = "Density",
subtitle = paste("n =", n, "| λ̂ =", signif(lambda_hat, 4),
"| x̄ =", signif(mean(x), 4))) +
theme_minimal(base_size = 14)

p_ecdf <- ggplot(data.frame(x), aes(x)) +
stat_ecdf(geom = "step") +
stat_function(fun = pexp, args = list(rate = lambda_hat), linetype = 2) +
labs(x = "Waiting time (min)", y = "ECDF") +
theme_minimal(base_size = 14)

p_hist / p_ecdf
```

Note: the exponential model assumes identical rate λ for all intervals, but the data exhibit two eruption regimes, which explains the slight departure from the exponential fit.

## Parameter Estimation

Let $X_1,\ldots,X_n \stackrel{\text{i.i.d.}}{\sim} \mathrm{Exp}(\lambda)$ with pdf $f(x)=\lambda e^{-\lambda x}$ for $x>0$. The parameter is the rate $\lambda>0$.

```{r}
n <- length(x)
xbar <- mean(x)
S <- sum(x)
```

### Method of Moments (MoM):
From $\mathbb{E}[X] = 1/\lambda$, matching $\bar{X}$ to $\mathbb{E}[X]$ gives 
  
```{r}
lambda_MoM <- 1 / xbar
lambda_MoM
```

### Maximum Likelihood Estimation (MLE):
Log-likelihood
  $$
    \ell(\lambda)= n\log\lambda - \lambda \sum_{i=1}^n x_i.
  $$
  Score eqn:
  $$
    \frac{d\ell}{d\lambda}=\frac{n}{\lambda}-\sum x_i=0
    \ \Rightarrow\ 
    \hat\lambda = \frac{1}{\bar X}.
  $$
  
```{r}
lambda_MLE <- 1 / xbar
lambda_MLE == lambda_MoM  # MoM = MLE for Exp
```

### Asymptotic standard error:
  $$
    \operatorname{SE}(\hat\lambda) \approx \frac{\hat\lambda}{\sqrt{n}}.
  $$
```{r}
se_lambda <- lambda_MLE / sqrt(n)
se_lambda
```


## Moments & Variance
Theoretical moments for $(X\sim\mathrm{Exp}(\lambda))$:
  $$
    \mathbb{E}[X] = \frac{1}{\lambda},\qquad
   \mathbb{E}[X^2] = \frac{2}{\lambda^2},\qquad
    \operatorname{Var}(X) = \frac{1}{\lambda^2}.
  $$

```{r}
E_X_hat  <- 1 / lambda_MLE
E_X2_hat <- 2 / (lambda_MLE^2)
Var_hat  <- 1 / (lambda_MLE^2)

list(E_X_hat = E_X_hat, E_X2_hat = E_X2_hat, Var_hat = Var_hat)
```

Sanity check for exponential data:
 $$
    \operatorname{Var}(X) \approx (\mathbb{E}[X])^2
    \quad\Rightarrow\quad
    \frac{s^2}{\bar X^{\,2}} \approx 1.
  $$
```{r}
var(x) / (mean(x)^2)
```

### Confidence Intervals

Chi-square pivot:
$$
2\lambda \sum X_i \;\sim\; \chi^2_{2n} \, 
$$

CI for $\lambda$:
$$
\left[
\frac{\chi^2_{\alpha/2,\,2n}}{2\sum X_i},\;
\frac{\chi^2_{1-\alpha/2,\,2n}}{2\sum X_i}
\right]
$$

CI for $\mu = 1/\lambda$:
$$
\left[
\frac{2\sum X_i}{\chi^2_{1-\alpha/2,\,2n}},\;
\frac{2\sum X_i}{\chi^2_{\alpha/2,\,2n}}
\right]
$$

```{r}
alpha <- 0.05
CI_lambda <- c(qchisq(alpha/2, 2*n)/(2*S), qchisq(1 - alpha/2, 2*n)/(2*S))
CI_mean   <- c(2*S/qchisq(1 - alpha/2, 2*n), 2*S/qchisq(alpha/2, 2*n))

list(CI_lambda_95 = CI_lambda, CI_mean_95 = CI_mean)
```

For an exponential distribution, the ratio of the sample variance to the squared sample mean should be close to 1. The much smaller observed value indicates that the waiting times are far less variable than expected under a true exponential process. This strongly rejects the assumption of a single, memoryless rate. The data exhibits two distinct eruption regimes (short and long waiting periods) producing a bimodal distribution rather than a single exponential pattern. Therefore, a single-rate exponential model is not an adequate fit for the faithful data. Better alternatives include a mixture of two exponential or gamma distributions or stratifying the data by eruption type (short vs. long).


## Formal Foundations
After all the suffering through theory we've done in the last few lessons, the formal theory needed for this lesson is quite light.


### Improper Integrals
The Fundamental Theorem of Calculus states that, if $f$ is a continuous function on a closed interval from $a$ to $b$ (inclusive) with anti-derivative $F$, then 
$$
\int_a^b f(x) dx \equiv \left[ F(x) \right]_a^b \equiv F(b) - F(a).
$$
Notice that this definition requires that the interval *include* both $a$ and $b$. [**Improper integrals**](https://www.sfu.ca/math-coursenotes/Math%20158%20Course%20Notes/sec_ImproperIntegrals.html) mean that either $f$ is not continuous over the interval $[a,b]$, or either $a$, $b$, or both, are infinite. For many statistical distributions, the bounds of the support of the random variable include $\infty$ on one side or both. That means that we can't just take the integral and substitute in $\infty$ and "call it a day". For common statistical distributions with unbounded support, there are two cases.

**Case 1: The Interval is Open on One Side**. If we have either the lower or upper bound of the integral tending to $\infty$, then we do this:
$$
\begin{aligned}
\int_a^{\infty} f(x) dx &= \lim_{\psi\to\infty} F(\psi) - F(a) \\
\int_{-\infty}^b f(x) dx &= F(b) - \lim_{\psi\to -\infty} F(\psi) \\
\end{aligned}
$$

**Case 2: The Interval is Open on Both Sides**. If we have the lower and upper bound of the integral as the entire Real line, then we split the integral into two "one-sided" improper integrals at some value $x = a$ and evaluate each separately. That is:
$$
\int_{-\infty}^{\infty} f(x) dx = \int_{-\infty}^a f(x) dx\ +\ \int_a^{\infty} f(x) dx.
$$
If $f$ is symmetric around the axis $x = a$, then this simplifies even further:
$$
\int_{-\infty}^{\infty} f(x) dx = 2 \times \int_a^{\infty} f(x) dx.
$$
The Normal and Student's $t$ distributions are the two most famous distributions with support over the entire Real line, and they are both symmetric around their mean values ($\mu$ for the Normal and 0 for the Student's $t$ distributions).

</br>



## Show that this is a Distribution
We recall that $\lambda, t > 0$, so $f_{\text{Exp}}(t|\lambda) > 0$. For the total probability, we need to remember that we can reverse the limits of integration,^[<https://proofwiki.org/wiki/Reversal_of_Limits_of_Definite_Integral>] and we need to be able to solve an **improper integral** (described above). Once you have reviewed these concepts, consider
$$
\begin{aligned}
\int_{\mathcal{S}(t)} dF(t|\lambda) &= \int_0^{\infty} \lambda e^{-\lambda t} dt \\
&= \lim_{\psi \to \infty} \left[ -\frac{\lambda}{\lambda} e^{-\lambda t} \right]_{t = 0}^{\psi} \\
&= \lim_{\psi \to \infty} \left[ -e^{-\lambda t} (-1) \right]_{\psi}^{t = 0} \\
&= \lim_{\psi \to \infty} \left[ e^{-\lambda t} \right]_{\psi}^{t = 0} \\
&= \left[ e^{\lambda [0]} \right] - \left[ \lim_{\psi \to \infty} e^{-\lambda \psi} \right] \\
&= 1 - 0.
\end{aligned}
$$
Therefore, $f_{\text{Exp}}(t|\lambda)$ is a probability distribution.


</br>



## Derive the Moment Generating Function
As with the Poisson Distribution, we will call the nuisance parameter for the MGF $s$ instead of $t$, as $t$ is the random variable of the Exponential Distribution. Recall that the MGF must be defined for $s$ in an $\epsilon$-neighbourhood of 0, for some arbitrarily small $\epsilon$. This means that, without loss of generality, we can bound $s$ to be smaller than the rate parameter $\lambda > 0$ (which we will need below). Thus,

$$
\begin{aligned}
M_t(s) &= \int_{\mathcal{S}(t)} e^{st} dF(t|\lambda) \\
&= \int_0^{\infty} e^{st} \lambda e^{-\lambda t} dt \\
&= \lambda \int_0^{\infty} e^{t(s - \lambda)} dt \\
&= \frac{\lambda}{s - \lambda} \lim_{\psi\to\infty} \left[ e^{t(s - \lambda)} \right]_{t = 0}^{\psi},\ s < \lambda \\
&= \frac{\lambda}{s - \lambda} \left[ \lim_{\psi\to\infty} e^{\psi(s - \lambda)} - e^{[0](s - \lambda)} \right] \\
&= \frac{\lambda}{s - \lambda} \left[ \lim_{\psi\to\infty} e^{-\psi(\lambda - s)} - 1 \right],\ s < \lambda \Rightarrow \lambda - s > 0 \\
&= \frac{\lambda}{s - \lambda} [0 - 1] \\
&= \frac{\lambda}{\lambda - s}.
\end{aligned}
$$

</br>



## Method of Moments Estimates from Observed Data
Let's generate some random data. We continue our "clinic" example, and now we generate 7 "waiting times" until the first patient walks in. For a single experiment, that is, when we first open the clinic, how long will we have to wait for the first patient to arrive? Let's assume the same rate of $\lambda = 5$ for one hour that we used in the previous lesson. We can generate data (in fractional hours) by
```{r}
set.seed(20150516)

nTrials_int <- 7L
rate_num <- 5
Expt_num <- rexp(n = nTrials_int, rate = rate_num)
Expt_num
Expt_num * 60
```
So, for these 7 independent trials where the waiting times $T$ have an identical Exponential distribution with rate of 5 patients per hour, we wait `r round(Expt_num * 60, 1)` minutes to see the first patient.


### $\mathbb{E}[t]$
Consider
$$
\begin{aligned}
M_t(s) &= \lambda(\lambda - s)^{-1} \\
\Longrightarrow M_t^{\prime}(s) &= \frac{d}{ds} \lambda(\lambda - s)^{-1} \\
&= -\lambda(\lambda - s)^{-2}(-1) \\
&= \lambda(\lambda - s)^{-2} \\
\Longrightarrow M_t^{\prime}(0) &= \frac{\lambda}{(\lambda - [0])^2} \\
&= \frac{1}{\lambda} \\
&= \mathbb{E}[t].
\end{aligned}
$$


### $\mathbb{E}[t^2]$ and $\text{Var}[t]$
We continue:
$$
\begin{aligned}
M^{\prime}_t(s) &= \lambda(\lambda - s)^{-2} \\
\Longrightarrow M^{\prime\prime}_t(s) &= \frac{d}{ds} \lambda(\lambda - s)^{-2} \\
&= -2\lambda(\lambda - s)^{-3}(-1) \\
&= \frac{2\lambda}{(\lambda - s)^3} \\
\Longrightarrow M^{\prime\prime}_t(0) &= \frac{2\lambda}{(\lambda - [0])^3} \\
&= \frac{2}{\lambda^2} \\
&= \mathbb{E}[t^2].
\end{aligned}
$$

Then,
$$
\text{Var}[t] = \mathbb{E}[t^2] - \left[\mathbb{E}[t]\right]^2 = \frac{2}{\lambda^2} - \left[\frac{1}{\lambda}\right]^2 = \frac{1}{\lambda^2}.
$$


### Solving the System
We then have that $\bar{x} = \frac{1}{\lambda}$ and $s^2 = \frac{1}{\lambda^2}$, which is an overdetermined system (with $\hat{\lambda}_{\text{MoM}} = \frac{1}{\bar{t}}$). For the Exponential Distribution, once we know the mean, then we should also know the variance. For our sample, generated from an Exponential with rate $\lambda = 5$, $\hat{\lambda}_{\text{MoM}}$ = `r round(1/mean(Expt_num), 3)`. It's worth noting that the Method of Moments estimate for this distribution requires a very large number of samples before it is "close" to the true value (the Maximum Likelihood estimator is the same, as we'll see next).


</br>



## Maximum Likelihood Estimators
To estimate a true rate, $\lambda$, using the likelihood, we collect a set of independent observed times for the first success. That is, $\textbf{t} = [t_1, t_2, \ldots, t_n] \overset{iid}{\sim} \text{Exp}(\lambda)$. Thus,
$$
\begin{aligned}
f_{\text{Exp}}(t|\lambda) &= \lambda e^{-\lambda t} \\
\Longrightarrow \mathcal{L}(\lambda|\textbf{t}) &= \prod_{i = 1}^n \lambda e^{-\lambda t_i} \\
\Longrightarrow \ell(\lambda|\textbf{t}) &= \log \left[ \prod_{i = 1}^n \lambda e^{-\lambda t_i} \right] \\
&= \sum_{i = 1}^n \log \left[ \lambda e^{-\lambda t_i} \right] \\
&= \sum_{i = 1}^n \left[ \log(\lambda) - \lambda t_i \right] \\
&= n\log(\lambda) - \lambda \sum_{i = 1}^n t_i \\
&= n\log(\lambda) - n\lambda\bar{t} \\
\Longrightarrow \frac{\partial}{\partial\lambda} \ell(\lambda|\textbf{t}) &= \frac{\partial}{\partial\lambda} \left( n\log(\lambda) - n\lambda\bar{t} \right) \\
&= \frac{n}{\lambda} - n\bar{t} \\
\Longrightarrow 0 &\overset{\text{set}}{=} \frac{n}{\hat{\lambda}} - n\bar{t} \\
\Longrightarrow n\bar{t} &= \frac{n}{\hat{\lambda}} \\
\Longrightarrow \hat{\lambda} &= \frac{1}{\bar{t}}.
\end{aligned}
$$
In order to confirm that this extreme value of the log-likelihood is truly a maxima, we take the second partial derivative:
$$
\begin{aligned}
\frac{\partial}{\partial\lambda} \ell(\lambda|\textbf{t}) &= \frac{n}{\lambda} - n\bar{t} \\
&= n\lambda^{-1} - n\bar{t} \\
\Longrightarrow \frac{\partial^2}{\partial\lambda^2} \ell(\lambda|\textbf{t}) &= -n\lambda^{-2} \\
&< 0.
\end{aligned}
$$
Hence, $\hat{\lambda}_{MLE} = \frac{1}{\bar{t}}$.

</br>



## Exercises

1. Show that the exponential distribution is a valid distribution by taking the limit of the CDF as $t \rightarrow \infty$

2.Show that if $X \sim \text{Unif}(0,1)$, then $Y = -\frac{1}{\lambda}\ln X \sim \text{Exp}(\lambda)$, where $\lambda > 0$

3. Using simulated or real waiting-time data, estimate the rate parameter $\lambda$ of an Exponential distribution, plot the histogram of the observed waiting times with the fitted Exponential curve overlaid, and comment on whether the model provides a good fit.

## Footnotes 


