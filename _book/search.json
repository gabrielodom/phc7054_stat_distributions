[
  {
    "objectID": "chapters/bernoulli_20250310.html",
    "href": "chapters/bernoulli_20250310.html",
    "title": "2  The Bernoulli Distribution",
    "section": "",
    "text": "2.1 Deriving the Distribution\nIn the mid-1600s, mathematicians like Pascal and Fermat were obsessed with games of chance.1 The simplest such game is flipping a single coin. Let \\(P[A]\\) denote the probability of event \\(A\\) occurring. Because flipping a coin has only two outcomes (heads or tails; we ignore the microscopic possibility of a coin landing on its edge for practical gambling scenarios), we can define \\(p \\equiv P[\\text{head}]\\), which necessarily implies that \\(1 - p = P[\\text{tails}]\\). For ease of notation, we let \\(k\\in\\{0,1\\} = 1\\) when the coin hands on leads and \\(k = 0\\) for tails. Thus, we define a Bernoulli Trial as one random value drawn from the following distribution: \\[\nf_{\\text{Bern}}(k|p) = p^k(1-p)^{1-k},\\ k\\in\\{0,1\\},\\ p \\in (0,1) \\subset \\mathbb{R}.\n\\]\nNotice a few things:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#deriving-the-distribution",
    "href": "chapters/bernoulli_20250310.html#deriving-the-distribution",
    "title": "2  The Bernoulli Distribution",
    "section": "",
    "text": "The Bernoulli Probability Mass Function is denoted \\(f_{\\text{Bern}}\\); \\(f\\) is a function, and its argument \\(k\\) is discrete. The domain of \\(f\\) is 0 or 1 (\\(k\\) can only have the values in the set \\(\\mathcal{S} = \\{0,1\\}\\)).\nFor any \\(k \\in \\mathcal{S}\\), \\(f(k|p) \\ge 0\\); this is the range of \\(f\\). This means that \\(f\\) maps from the set \\(\\mathcal{S}\\) to the set of all non-negative real numbers, which is symbolically denoted as \\(f:\\mathcal{S} \\to \\mathbb{R}_{\\ge}\\).\nThe probability of a “head” (success) is the only parameter of \\(f\\), and it is fixed at some value \\(p\\), which must be a real number between 0 and 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#example-random-samples",
    "href": "chapters/bernoulli_20250310.html#example-random-samples",
    "title": "2  The Bernoulli Distribution",
    "section": "2.2 Example Random Samples",
    "text": "2.2 Example Random Samples\nWe now take some random samples from this distribution when \\(p = 0.5\\).\n\n\nCode\nset.seed(20150516)\n\nx &lt;- rbinom(n = 100, size = 1, prob = 0.5)\nsamples_ls &lt;- list(\n  n5   = x[1:5],\n  n15  = x[1:15],\n  n30  = x[1:30],\n  n100 = x\n)\n\nrm(x)\n\n\n\n\nCode\npar(mfrow = c(2, 2))\n\nhist(samples_ls$n5)\nhist(samples_ls$n15)\nhist(samples_ls$n30)\nhist(samples_ls$n100)\n\npar(mfrow = c(1, 1))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#show-that-this-is-a-distribution",
    "href": "chapters/bernoulli_20250310.html#show-that-this-is-a-distribution",
    "title": "2  The Bernoulli Distribution",
    "section": "2.3 Show that this is a Distribution",
    "text": "2.3 Show that this is a Distribution\n\n2.3.1 Review: Properties of Distributions\nLet \\(x\\) be an observed value \\(\\in \\mathcal{A}\\), and let \\(\\boldsymbol\\theta\\) be a vector of parameters in a parameter space \\(\\boldsymbol\\Theta \\subseteq \\mathbb{R}^q\\). Consider a function \\(f(\\textbf{x}|\\boldsymbol\\theta)\\) with anti-derivative \\(F\\), and note that \\(f\\) need not be continuous. This \\(f\\) represents a probability distribution iff2\n\n\\(\\forall x \\in \\mathcal{S}\\), \\(\\forall \\boldsymbol\\theta \\in \\boldsymbol\\Theta\\), \\(f(x|\\boldsymbol\\theta) \\ge 0\\).\n\\(\\forall \\boldsymbol\\theta \\in \\boldsymbol\\Theta\\), \\(\\int_{x \\in \\mathcal{S}} \\text{d}F(x|\\boldsymbol\\theta) = 1\\), where \\(\\text{d}F\\) is the integrand of a Riemann-Stieltjes integral.\n\n\nThe Riemann-Stieltjes Integral\nLet \\(f\\) be a bounded function on the interval \\(\\mathcal{S} = [a, b] \\subset \\mathbb{R}\\), and let \\(G\\) be a monotone increasing (but not necessarily continuous) function on \\(\\mathcal{S}\\). The Riemann-Stieltjes integral of \\(f\\) with respect to \\(G\\) is denoted as \\[\n\\text{R-S}(f, G) \\equiv \\int_{x \\in \\mathcal{S}} f(x) \\text{d}G(x).\n\\] If \\(G\\) is continuous \\(\\forall x \\in \\mathcal{S}\\), then this integral simplifies to \\[\n\\int_{x \\in \\mathcal{S}} f(x) \\text{d}G(x) = \\int_{x \\in \\mathcal{S}} f(x) G^{\\prime}(x).\n\\] If, however, there exists \\(k &lt; m &lt; \\infty\\) points of discontinuity for \\(G\\) on \\(\\mathcal{S}\\), we define an \\(m\\)-partition of \\(\\mathcal{S}\\) as \\(\\{[y_0, y_1), [y_1, y_2), \\ldots, [y_{m - 2}, y_{m - 1}), [y_{m - 1}, y_m]\\}\\), where \\(\\{a = y_0, b = y_m\\}\\) and the \\(k\\) points of discontinuity are included in the sequence \\(\\{y_1, y_2, \\ldots, y_{m - 1}\\}\\). Then, this integral simplifies to \\[\n\\int_{x \\in \\mathcal{S}} f(x) \\text{d}G(x) = \\sum\\limits_{i = 1}^m f(x)\\left[ G(y_i) - G(y_{i - 1}) \\right].\n\\]\n\nAs long as (1) holds above, then \\(F\\) will be monotone increasing (because the anti-derivative of a non-negative function will always be flat or increasing). The probability density/mass functions for all statistical distributions share these two properties above. Because of the flexibility of the Riemann-Stieltjes integral, we don’t have to make the distinction between probability density functions and probability mass functions any longer. This is because\n\nIf \\(\\mathcal{S}\\) is a discrete set with cardinality \\(|\\mathcal{S}| = n\\), \\(f\\) is commonly referred to as a probability “mass” function. Then, because (1) holds, \\(\\exists\\) some ordering of the elements of \\(\\mathcal{S} \\ni 0 \\le F(x^{(1)}) \\le F(x^{(2)}) \\le \\cdots \\le F(x^{(n)}) \\le 1\\). We know that the total probability of all events is 1, and the total probability of no events is 0, so, by convention, we let \\(F(x^{(n)}) = 1\\) and \\(F(x^{(0)}) = 0\\). Thus, noticing the Telescoping Series3, \\[\n\\int_{x \\in \\mathcal{S}} \\text{d}F(x|\\boldsymbol\\theta) = \\sum\\limits_{i = 1}^n F(x^{(i)}) - F(x^{(i - 1)}) = F(x^{(n)}) - F(x^{(0)}) = 1.\n\\]\nIf \\(\\mathcal{S} = [a,b]\\) is a continuous set, then \\(f\\) is a probability “density” function. For this continuous range, \\(F(a) = 0\\) and \\(F(b) = 1\\). Thus, \\[\n\\int_{x \\in \\mathcal{S}} \\text{d}F(x|\\boldsymbol\\theta) = \\int_a^b F^{\\prime}(x) = F(b) - F(a) = 1.\n\\]\n\nThus, for the remainder of these notes, we will start all integral-based definitions with the Riemann-Stieltjes form, and then reduce this form into traditional sums or integrals, as is appropriate for the distribution at hand.\n\n\n2.3.2 Properties of the Bernoulli Distribution\nGiven the extensive review above, showing that \\(f_{\\text{Bern}}\\) is a probability distribution is anti-climactic.\nClaim 1: The function \\(f_{\\text{Bern}}\\) must be non-negative for all values of its support given \\(p\\) in the parameter space \\((0,1)\\).\nArgument 1: If \\(k = 0\\), then \\(f_{\\text{Bern}}(k = 0|p) = p^0(1 - p)^1 = 1 - p \\ge 0\\). Similarly, if \\(k = 1\\), then \\(f_{\\text{Bern}}(k = 1|p) = p^1(1 - p)^0 = p \\ge 0\\).\nClaim 2: The integral of the function \\(f_{\\text{Bern}}\\) over all possible values of \\(k\\) must be 1.\nArgument 2: Consider that \\[\n\\begin{aligned}\n\\int_{x \\in \\mathcal{S}} \\text{d}F(x|\\boldsymbol\\theta) &= \\sum\\limits_{k = 0}^1 f_{\\text{Bern}}(k|p) \\\\\n  &= \\left[ p^k(1-p)^{1-k} \\right]_{k = 0} + \\left[ p^k(1-p)^{1-k} \\right]_{k = 1} \\\\\n  &= [p^0(1-p)^1] + [p^1(1-p)^0] \\\\\n  &= (1 - p) + p \\\\\n  &= 1.\n\\end{aligned}\n\\]\n\n\n2.3.3 An Example Sample from the Bernoulli Distribution\nNow let’s use R to take \\(n = 100\\) random samples from a Bernoulli Distribution with \\(p = 0.35\\), but we will only inspect the first five (for now):\n\nmyBernSample &lt;- rbinom(n = 100, size = 1, prob = 0.35)\nmyBernSample[1:5]\n[1] 1 0 1 0 0\n\nWe now pretend that we only know the results for the first five coin flips. We have flipped one coin five times, with the results \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#derive-the-moment-generating-function",
    "href": "chapters/bernoulli_20250310.html#derive-the-moment-generating-function",
    "title": "2  The Bernoulli Distribution",
    "section": "2.4 Derive the Moment Generating Function",
    "text": "2.4 Derive the Moment Generating Function\n\n2.4.1 Review: What is the MGF?\nThe Moment Generating Function4 (MGF) is, as its name implies, a function to “generate” (i.e., calculate) moments. In statistics, I have not found great intuition on what a “moment” is, other than it relates to various measures of a probability distribution:\n\nThe 0\\(^{\\text{th}}\\) moment is the total area of the probability distribution [or 1],\nThe 1\\(^{\\text{st}}\\) moment is the expected value,\nThe 2\\(^{\\text{nd}}\\) (central) moment is the variance,\nThe 3\\(^{\\text{rd}}\\) moment is the skewness, and\nThe 4\\(^{\\text{th}}\\) moment is the kurtosis.\n\nPhysics has more intuition of moments, where the 1\\(^{\\text{st}}\\) moment is the center of mass for a body (the point at which you could balance the shape on a pencil) and the 2\\(^{\\text{nd}}\\) moment is the moment of inertia (how much mass is spread out away from the axis at the center of mass, where larger values mean the mass is spread out further away from the first moment).\nGiven a Cumulative Density Function \\(F_X(x|\\boldsymbol\\theta)\\), the MGF of \\(F_X\\) with respect to some value \\(t\\) in an \\(\\epsilon\\)-neighborhood5 of 0 is defined to be \\[\nM_X(t) \\equiv \\mathbb{E}\\left[ e^{tX} \\right] = \\int\\limits_{x \\in \\mathcal{S}(X)} e^{tx} \\text{d}F_X(x|\\boldsymbol\\theta),\n\\] where \\(\\text{d}F_X\\) is the integrand of a Riemann-Stieltjes integral (as discussed above).\nIf we have a distribution with \\(j\\) parameters, the process to calculate the first \\(j\\) moments is to take the first \\(j\\) derivatives of \\(M_X\\) and evaluate these functions (if they exist) at \\(t = 0\\). Then, these theoretical moments (functions of the distribution’s parameters \\(\\boldsymbol\\theta\\)) are set equal to the first \\(j = |\\boldsymbol\\theta|\\) sample moments, yielding a system of (often non-linear) equations to solve.\n\n\n2.4.2 MGF of the Bernoulli Distribution\nGiven the definition above, we can calculate the MGF: \\[\n\\begin{aligned}\nM_K(t) &\\equiv \\mathbb{E}\\left[ e^{tK} \\right] \\\\\n  &= \\int\\limits_{k \\in \\{0,1\\}} e^{tk} \\text{d}F_K(k|p) \\\\\n  &= \\sum\\limits_{k = 0}^1 e^{tk} p^k (1 - p)^{1 - k} \\\\\n  &= \\sum\\limits_{k = 0}^1 (pe^t)^k (1 - p)^{1 - k} \\\\\n  &= \\left[ (pe^t)^0 (1 - p)^1 \\right] + \\left[ (pe^t)^1 (1 - p)^0 \\right] \\\\\n  &= 1 - p + pe^t.\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#method-of-moments-estimators",
    "href": "chapters/bernoulli_20250310.html#method-of-moments-estimators",
    "title": "2  The Bernoulli Distribution",
    "section": "2.5 Method of Moments Estimators",
    "text": "2.5 Method of Moments Estimators\nNow that we have the MGF of the Bernoulli Distribution, we follow the process to calculate the theoretical moment(s) and set them equal to their corresponding sample moment(s). Because the Bernoulli Distribution only has \\(j = 1\\) parameter, our systems to solve will be somewhat trivial.\n\n2.5.1 First Moment\nGiven the MGF calculated above, we begin with \\[\n\\begin{aligned}\nM_K(t) &= 1 - p + pe^t \\\\\n\\Longrightarrow\\qquad \\frac{\\partial}{\\partial t}M_K(t) &= pe^t \\\\\n\\Longrightarrow\\qquad \\frac{\\partial}{\\partial t}M_K(0) &= p \\\\\n&= \\mathbb{E}[X].\n\\end{aligned}\n\\]\n\n\n2.5.2 The Second Moment\nThe second (non-central) moment is then \\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial t}M_K(t) &= pe^t \\\\ \\\\\n\\Longrightarrow\\qquad \\frac{\\partial^2}{\\partial t^2}M_K(t) &= pe^t \\\\\n\\Longrightarrow\\qquad \\frac{\\partial^2}{\\partial t^2}M_K(0) &= p \\\\\n&= \\mathbb{E}[X^2].\n\\end{aligned}\n\\]\nBecause this is the non-central moment, we find the second central moment by exploiting the common relationship between variance and moments; that is, \\[\n\\begin{aligned}\n\\text{Var}[X] &= \\mathbb{E}[X^2] - \\mathbb{E}^2[X] \\\\\n&= p - [p]^2 \\\\\n&= p(1 - p).\n\\end{aligned}\n\\]\n\n\n2.5.3 Solving the System\n\nFor distributions with two parameters, we would now equate these two population moments, \\(\\mathbb{E}[X]\\) and \\(\\text{Var}[X]\\), the two sample moments, \\(\\bar{x}\\) and \\(s^2\\), to yield the Method of Moments estimators for the two parameters of the distribution (which we will represent generically as \\(\\hat\\theta_{MoM}\\) and \\(\\hat\\phi_{MoM}\\)).\n\nHowever, the Bernoulli has only one parameter, so our “system” of equations is just \\[\n\\begin{aligned}\n\\bar{x} &= p \\\\\n\\Longrightarrow \\hat{p}_{MoM} &= \\bar{x}.\n\\end{aligned}\n\\] For the data observed above, the coin-flip results \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\) which came from a coin with \\(P[\\text{Heads}] = 0.35\\), \\(\\hat{p}_{MoM} = \\frac{1}{5}\\sum_{i = 1}^5 x_i = 0.4\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#maximum-likelihood-estimators",
    "href": "chapters/bernoulli_20250310.html#maximum-likelihood-estimators",
    "title": "2  The Bernoulli Distribution",
    "section": "2.6 Maximum Likelihood Estimators",
    "text": "2.6 Maximum Likelihood Estimators\nIn standard statistical inference, we assume that the population parameter is some fixed but unknown value and that our observed data are random; we must estimate the unknown (but fixed) parameter using statistics of the observed (but random) data. Likelihood functions (and the related Bayesian school of thought) turn this question “inside out”. The likelihood instead assumes that the parameter is a random variable (still unknown), but that the data are both observed and fixed.\nLet’s recall the observed toy data: we flipped one coin five times and observed \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\). Is the coin fair? Traditional statistics would say “\\(p\\) is unknown, but if I can collect enough data then I can estimate it with a narrow confidence interval”. In a simple coin-flip example, this is reasonable: we can hire a person to repeatedly flip that same coin for hours and hours and record the results. Eventually, we will have a sample size large enough so that we can build a confidence interval small enough to say with any requested level of “confidence” that the coin is or isn’t fair.\nRecall the \\((1-\\alpha)\\)-level confidence interval for a Bernoulli \\(p\\) is \\[\n\\text{CI}(p|n,\\alpha) =\n  \\hat{p} \\pm \\frac{z_{\\alpha/2}}{\\sqrt{n}}\\sqrt{\\hat{p}(1 - \\hat{p})},\n\\] where \\(z_{\\alpha/2}\\) is the Standard Normal \\(z\\) corresponding to the quantiles \\(1 - \\alpha/2\\) and \\(\\alpha/2\\). As we saw above, the true parameter value was \\(p = 0.35\\), which (we remark) would be impossible to know in real life.\nWe will assume that we take each coin flip one at a time. How many times will we have to flip the coin before the 95% CI no longer contains 0.5?\n\n\nCode\nzAlpha &lt;- qnorm(p = 0.975)\nresults_ls &lt;- lapply(\n  X = seq.int(from = 2, to = length(myBernSample), by = 1),\n  FUN = function(n_i) {\n    \n    x &lt;- myBernSample[1:n_i]\n    pHat &lt;- mean(x)\n    CIwidth &lt;- ( zAlpha / sqrt(n_i) ) * sqrt( pHat * (1 - pHat) )\n    \n    data.frame(\n      N = n_i,\n      pHat = pHat,\n      CIlb = pHat - CIwidth,\n      CIub = pHat + CIwidth\n    )\n  }\n)\n\nresults_df &lt;- do.call(rbind, results_ls)\n\n# add trivial CI results for first coin flip\nresults_df &lt;- rbind(\n  data.frame(\n    N = 1, pHat = myBernSample[1],\n    CIlb = myBernSample[1], CIub = myBernSample[1]\n  ),\n  results_df\n)\n\n\nLet’s plot these results:\n\n\nCode\nlibrary(ggplot2)\nggplot(data = results_df[1:100, ]) + \n  aes(x = N) +\n  ylim(c(0, 1)) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 10)) +\n  geom_abline(slope = 0, intercept = 0.5, colour = \"red\") + \n  geom_line(aes(y = CIlb)) + \n  geom_line(aes(y = CIub))\n\n\n\n\n\n\n\n\n\nAs we can see, we need between 85-90 coin flips before we can be 95% sure that the coin is “not fair” (that the true value of \\(p\\ne 0.5\\)). Also, we see that the confidence intervals are degenerate for a few of the samples with \\(n\\le 5\\) (that the bounds of the confidence interval for \\(p\\) are outside \\([0,1]\\), which is impossible).\nFor the computer, flipping additional coins to create new data is trivially inexpensive. However, in real life, collecting one additional sample may be of enormous cost to the research team. In these cases, it doesn’t help us that we would theoretically be able to reject the claim that \\(p = 0.5\\) at some point in the future (with more samples), we need to be able to make a statement about the likelihood that the coin is fair with the samples we have right now. Let the event \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\) be encoded \\(\\textbf{x} = (1,0,1,0,0)\\). Thus, we apply the multiplication rule of independent events for these five coin flips, and define the likelihood function of \\(p\\) given the observed data:\n\\[\\begin{align}\n\\mathcal{L}(p|\\textbf{x}) &=\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 1} \\times\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 0} \\times \\\\\n  &\\qquad \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 1} \\times\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 0} \\times\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 0} \\\\\n  \n  &= \\left[ p^1 \\right] \\times \\left[ (1 - p)^1 \\right] \\times\n  \\left[ p^1 \\right] \\times \\left[ (1 - p)^1 \\right] \\times \\left[ (1 - p)^1 \\right] \\\\\n  &= p^2(1 - p)^3 \\\\\n  &= p^2(1 - 3p + 3p^2 - p^3) \\\\\n  &= p^2 - 3p^3 + 3p^4 - p^5.\n\\end{align}\\]\nThis function \\(\\mathcal{L}\\) contains almost all the information we have about \\(p\\): it has all the data, and it has our best guess about the data generating process (we think it’s a Bernoulli trial). A few things to notice:\n\n\\(\\mathcal{L}\\) is a function of the parameter, \\(p\\), not of the data.\n\\(\\mathcal{L}\\) is NOT a probability function; even though \\(\\mathcal{L} \\ge 0\\ \\forall p\\), we have that the integral over the support of \\(p\\) is\n\n\\[\\begin{align}\n\\int_0^1 \\mathcal{L}(p|\\textbf{x})dp &=\n  \\int_0^1 \\left[ p^2 - 3p^3 + 3p^4 - p^5 \\right]dp \\\\\n  &= \\frac{1}{3}p^3 - \\frac{3}{4}p^4 + \\frac{3}{5}p^5 - \\frac{1}{6}p^6 \\Big\\rvert_0^1 \\\\\n  &= \\frac{1}{3} - \\frac{3}{4} + \\frac{3}{5} - \\frac{1}{6} \\\\\n  &= \\frac{20}{60} - \\frac{45}{60} + \\frac{36}{60} - \\frac{10}{60} \\\\\n  &= \\frac{1}{60} \\\\\n  &\\ne 1\n\\end{align}\\]\nThis exercise serves two purposes: first to show that \\(\\mathcal{L}\\) is not a probability distribution, and second to show the value of the multiplicative constant which would make a distribution. That is, \\(f(p|\\textbf{x}) = 60*\\mathcal{L}(p|\\textbf{x})\\) is a probability distribution. Now we can make probabilistic statements about the value of \\(p\\).\nWe can see what the probability distribution function looks like:\n\n\nCode\np &lt;- seq(from = 0, to = 1, length.out = 101)\nf_p &lt;- 60 * p^2 * (1 - p)^3\n\nplot(x = p, y = f_p)\n\n\n\n\n\n\n\n\n\nAnd since we have calculated the indefinite integral already, we can plot the Cumulative Distribution Function:\n\n\nCode\nF_p &lt;- 60 * (p^3/3 - 3*p^4/4 + 3*p^5/5 - p^6/6)\nplot(x = p, y = F_p)\n\n\n\n\n\n\n\n\n\nFinally, we can make statements about the claim that \\(p = 0.5\\). For instance, what is \\(P[p &lt; 0.5]\\)?\n\nF_p[which(p == 0.5)]\n[1] 0.656\n\nWhat is \\(P[p &gt; 0.5]\\)?\n\n1 - F_p[which(p == 0.5)]\n[1] 0.344\n\nWhat is \\(P[0.4 &lt; p &lt; 0.6]\\)?\n\nF_p[which(p == 0.6)] - F_p[which(p == 0.4)]\n[1] 0.365\n\nWhat is an 80% credible set6 for \\(p\\)?\n\n# Lower\np[max(which(F_p &lt; 0.1))]\n[1] 0.2\n# Upper\np[min(which(F_p &gt; 0.9))]\n[1] 0.67",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#exercises",
    "href": "chapters/bernoulli_20250310.html#exercises",
    "title": "2  The Bernoulli Distribution",
    "section": "2.8 Exercises",
    "text": "2.8 Exercises\nTo be determined.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#footnotes",
    "href": "chapters/bernoulli_20250310.html#footnotes",
    "title": "2  The Bernoulli Distribution",
    "section": "",
    "text": "https://www.usu.edu/math/schneit/StatsStuff/Probability/probability2.html↩︎\nif and only if↩︎\n(https://en.wikipedia.org/wiki/Telescoping_series)↩︎\nhttps://en.wikipedia.org/wiki/Moment-generating_function↩︎\n\\(t \\in (-\\epsilon, \\epsilon) \\subset \\mathbb{R}\\) (where \\(\\epsilon\\) is an arbitrarily small value)↩︎\nWe could also notice that this is the kernel of a Beta distribution with \\(\\alpha = 3\\) and \\(\\beta = 4\\), with normalizing constant \\(\\frac{(3-1)!(4-1)!}{(3+4-1)!} = \\frac{2}{6*5*4} = \\frac{1}{60}\\), but that would make the next steps too easy…↩︎\nhttps://en.wikipedia.org/wiki/Credible_interval↩︎\nhttps://tutorial.math.lamar.edu/classes/calci/shapeofgraphptii.aspx↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html",
    "href": "chapters/binomial_20250310.html",
    "title": "3  The Binomial Distribution",
    "section": "",
    "text": "3.1 Deriving the Distribution",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#example-random-samples",
    "href": "chapters/binomial_20250310.html#example-random-samples",
    "title": "3  The Binomial Distribution",
    "section": "3.2 Example Random Samples",
    "text": "3.2 Example Random Samples\n\n\nCode\nset.seed(20150516)\n\nN &lt;- 10\nbins_int &lt;- seq.int(from = -1, to = N, by = 1)\n\nxSymm &lt;- rbinom(n = 100, size = N, prob = 0.5)\nsamplesSymm_ls &lt;- list(\n  n5   = xSymm[1:5],\n  n15  = xSymm[1:15],\n  n30  = xSymm[1:30],\n  n100 = xSymm\n)\n\nxSkew &lt;- rbinom(n = 100, size = N, prob = 0.2)\nsamplesSkew_ls &lt;- list(\n  n5   = xSkew[1:5],\n  n15  = xSkew[1:15],\n  n30  = xSkew[1:30],\n  n100 = xSkew\n)\n\nrm(xSymm, xSkew)\n\n\n\n\nCode\npar(mfrow = c(2, 2))\n\nhist(samplesSymm_ls$n5, breaks = bins_int)\nhist(samplesSymm_ls$n15, breaks = bins_int)\nhist(samplesSymm_ls$n30, breaks = bins_int)\nhist(samplesSymm_ls$n100, breaks = bins_int)\n\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 2))\n\nhist(samplesSkew_ls$n5, breaks = bins_int)\nhist(samplesSkew_ls$n15, breaks = bins_int)\nhist(samplesSkew_ls$n30, breaks = bins_int)\nhist(samplesSkew_ls$n100, breaks = bins_int)\n\npar(mfrow = c(1, 1))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#show-that-this-is-a-distribution",
    "href": "chapters/binomial_20250310.html#show-that-this-is-a-distribution",
    "title": "3  The Binomial Distribution",
    "section": "3.3 Show that this is a Distribution",
    "text": "3.3 Show that this is a Distribution",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#derive-the-moment-generating-function",
    "href": "chapters/binomial_20250310.html#derive-the-moment-generating-function",
    "title": "3  The Binomial Distribution",
    "section": "3.4 Derive the Moment Generating Function",
    "text": "3.4 Derive the Moment Generating Function",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#method-of-moments-estimators",
    "href": "chapters/binomial_20250310.html#method-of-moments-estimators",
    "title": "3  The Binomial Distribution",
    "section": "3.5 Method of Moments Estimators",
    "text": "3.5 Method of Moments Estimators",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#maximum-likelihood-estimators",
    "href": "chapters/binomial_20250310.html#maximum-likelihood-estimators",
    "title": "3  The Binomial Distribution",
    "section": "3.6 Maximum Likelihood Estimators",
    "text": "3.6 Maximum Likelihood Estimators",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#exercises",
    "href": "chapters/binomial_20250310.html#exercises",
    "title": "3  The Binomial Distribution",
    "section": "3.7 Exercises",
    "text": "3.7 Exercises",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#the-likelihood-function",
    "href": "chapters/bernoulli_20250310.html#the-likelihood-function",
    "title": "2  The Bernoulli Distribution",
    "section": "2.6 The Likelihood Function",
    "text": "2.6 The Likelihood Function\nIn standard statistical inference, we assume that the population parameter is some fixed but unknown value and that our observed data are random; we must estimate the unknown (but fixed) parameter using statistics of the observed (but random) data. Likelihood functions (and the related Bayesian school of thought) turn this question “inside out”. The likelihood instead assumes that the parameter is a random variable (still unknown), but that the data are both observed and fixed.\n\n2.6.1 Reviewing the Repeated Sampling Paradigm\nLet’s recall the observed toy data: we flipped one coin five times and observed \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\). Is the coin fair? Traditional statistics would say “\\(p\\) is unknown, but if I can collect enough data then I can estimate it with a narrow confidence interval”. In a simple coin-flip example, this is reasonable: we can hire a person to repeatedly flip that same coin for hours and hours and record the results. Eventually, we will have a sample size large enough so that we can build a confidence interval small enough to say with any requested level of “confidence” that the coin is or isn’t fair.\nRecall the \\((1-\\alpha)\\)-level confidence interval for a Bernoulli \\(p\\) is \\[\n\\text{CI}(p|n,\\alpha) =\n  \\hat{p} \\pm \\frac{z_{\\alpha/2}}{\\sqrt{n}}\\sqrt{\\hat{p}(1 - \\hat{p})},\n\\] where \\(z_{\\alpha/2}\\) is the Standard Normal \\(z\\) corresponding to the quantiles \\(1 - \\alpha/2\\) and \\(\\alpha/2\\). As we saw above, the true parameter value was \\(p = 0.35\\), which (we remark) would be impossible to know in real life.\nWe will assume that we take each coin flip one at a time. How many times will we have to flip the coin before the 95% CI no longer contains 0.5?\n\n\nCode\nzAlpha &lt;- qnorm(p = 0.975)\nresults_ls &lt;- lapply(\n  X = seq.int(from = 2, to = length(myBernSample), by = 1),\n  FUN = function(n_i) {\n    \n    x &lt;- myBernSample[1:n_i]\n    pHat &lt;- mean(x)\n    CIwidth &lt;- ( zAlpha / sqrt(n_i) ) * sqrt( pHat * (1 - pHat) )\n    \n    data.frame(\n      N = n_i,\n      pHat = pHat,\n      CIlb = pHat - CIwidth,\n      CIub = pHat + CIwidth\n    )\n  }\n)\n\nresults_df &lt;- do.call(rbind, results_ls)\n\n# add trivial CI results for first coin flip\nresults_df &lt;- rbind(\n  data.frame(\n    N = 1, pHat = myBernSample[1],\n    CIlb = myBernSample[1], CIub = myBernSample[1]\n  ),\n  results_df\n)\n\n\nLet’s plot these results:\n\n\nCode\nlibrary(ggplot2)\nggplot(data = results_df) + \n  aes(x = N) +\n  ylim(c(0, 1)) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 10)) +\n  geom_abline(slope = 0, intercept = 0.5, colour = \"red\") + \n  geom_line(aes(y = CIlb)) + \n  geom_line(aes(y = CIub))\n\n\n\n\n\n\n\n\n\nAs we can see, we need between 85-90 coin flips before we can be 95% sure that the coin is “not fair” (that the true value of \\(p\\ne 0.5\\)). Also, we see that the confidence intervals are degenerate for a few of the samples with \\(n\\le 5\\) (that the bounds of the confidence interval for \\(p\\) are outside \\([0,1]\\), which is impossible).\n\n\n2.6.2 Making the Most Use of our Data\nFor the computer, flipping additional coins to create new data is trivially inexpensive. However, in real life, collecting one additional sample may be of enormous cost to the research team. In these cases, it doesn’t help us that we would theoretically be able to reject the claim that \\(p = 0.5\\) at some point in the future (with more samples), we need to be able to make a statement about the likelihood that the coin is fair with the samples we have right now. Let the event \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\) be encoded \\(\\textbf{x} = (1,0,1,0,0)\\). Thus, we apply the multiplication rule of independent events for these five coin flips, and define the likelihood function of \\(p\\) given the observed data:\n\\[\\begin{align}\n\\mathcal{L}(p|\\textbf{x}) &=\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 1} \\times\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 0} \\times \\\\\n  &\\qquad \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 1} \\times\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 0} \\times\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 0} \\\\\n  \n  &= \\left[ p^1 \\right] \\times \\left[ (1 - p)^1 \\right] \\times\n  \\left[ p^1 \\right] \\times \\left[ (1 - p)^1 \\right] \\times \\left[ (1 - p)^1 \\right] \\\\\n  &= p^2(1 - p)^3 \\\\\n  &= p^2(1 - 3p + 3p^2 - p^3) \\\\\n  &= p^2 - 3p^3 + 3p^4 - p^5.\n\\end{align}\\]\n\n\n2.6.3 Integrating the Likelihood\nThis function \\(\\mathcal{L}\\) contains almost all the information we have about \\(p\\): it has all the data, and it has our best guess about the data generating process (we think it’s a Bernoulli trial). A few things to notice:6\n\n\\(\\mathcal{L}\\) is a function of the parameter, \\(p\\), not of the data.\n\\(\\mathcal{L}\\) is NOT a probability function; even though \\(\\mathcal{L} \\ge 0\\ \\forall p\\), we have that the integral over the support of \\(p\\) is\n\n\\[\\begin{align}\n\\int_0^1 \\mathcal{L}(p|\\textbf{x})dp &=\n  \\int_0^1 \\left[ p^2 - 3p^3 + 3p^4 - p^5 \\right]dp \\\\\n  &= \\frac{1}{3}p^3 - \\frac{3}{4}p^4 + \\frac{3}{5}p^5 - \\frac{1}{6}p^6 \\Big\\rvert_0^1 \\\\\n  &= \\frac{1}{3} - \\frac{3}{4} + \\frac{3}{5} - \\frac{1}{6} \\\\\n  &= \\frac{20}{60} - \\frac{45}{60} + \\frac{36}{60} - \\frac{10}{60} \\\\\n  &= \\frac{1}{60} \\\\\n  &\\ne 1\n\\end{align}\\]\nThis exercise serves two purposes: first to show that \\(\\mathcal{L}\\) is not a probability distribution, and second to show the value of the multiplicative constant which would make a distribution. That is, \\(f(p|\\textbf{x}) = 60*\\mathcal{L}(p|\\textbf{x})\\) is a probability distribution. Now we can make probabilistic statements about the value of \\(p\\).\nWe can see what the probability distribution function looks like (it should look like a Beta distribution, because it is):\n\n\nCode\np &lt;- seq(from = 0, to = 1, length.out = 101)\nf_p &lt;- 60 * p^2 * (1 - p)^3\n\nplot(x = p, y = f_p)\n\n\n\n\n\n\n\n\n\nAnd since we have calculated the indefinite integral already, we can plot the Cumulative Distribution Function:\n\n\nCode\nF_p &lt;- 60 * (p^3/3 - 3*p^4/4 + 3*p^5/5 - p^6/6)\nplot(x = p, y = F_p)\n\n\n\n\n\n\n\n\n\nFinally, we can make statements about the claim that \\(p = 0.5\\). For instance, what is \\(P[p &lt; 0.5]\\)?\n\nF_p[which(p == 0.5)]\n[1] 0.656\n\nWhat is \\(P[p &gt; 0.5]\\)?\n\n1 - F_p[which(p == 0.5)]\n[1] 0.344\n\nWhat is \\(P[0.4 &lt; p &lt; 0.6]\\)?\n\nF_p[which(p == 0.6)] - F_p[which(p == 0.4)]\n[1] 0.365\n\nWhat is an 80% credible set7 for \\(p\\)?\n\n# Lower\np[max(which(F_p &lt; 0.1))]\n[1] 0.2\n# Upper\np[min(which(F_p &gt; 0.9))]\n[1] 0.67",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#maximum-likelihood-estimators-the-most-likely-value-of-p",
    "href": "chapters/bernoulli_20250310.html#maximum-likelihood-estimators-the-most-likely-value-of-p",
    "title": "2  The Bernoulli Distribution",
    "section": "2.7 Maximum Likelihood Estimators: The “Most Likely” Value of \\(p\\)",
    "text": "2.7 Maximum Likelihood Estimators: The “Most Likely” Value of \\(p\\)\nWe have an integrated likelihood, which contains basically almost all there is to know about the data we’ve collected, but finding a closed form of the integral of \\(\\mathcal{L}\\) (necessary to find \\(f\\) and \\(F\\)) can be impossible in most real-world scenarios. Rather than trying to answer all the questions about \\(p\\), sometimes it’s still worthwhile to answer “what is the most likely value of \\(p\\) given the data we’ve observed?”\nThis is answered with maximum likelihood estimation, and we need two steps. Using (multivariable) differential calculus, we\n\nfind the value of \\(\\boldsymbol\\theta\\) which maximizes \\(\\mathcal{L}(\\boldsymbol\\theta|\\textbf{x})\\), and\nshow that \\(\\mathcal{L}(\\boldsymbol\\theta|\\textbf{x})\\) is concave down8 at this point.\n\nFor the first step, it is common practice to 1) disregard any multiplicative constants leading \\(\\mathcal{L}\\) (because the derivative in the next step will zero these constants out) and 2) to take the natural logarithm of the likelihood and maximize that instead. Because logarithms simply change the scale of the vertical axis, they do not affect the location of extreme values. Let’s begin (I show what happens to the multiplicative constant in square brackets):\n\\[\\begin{align}\n\\mathcal{L}(p|\\textbf{x}) &= [60\\times]\\ p^2(1 - p)^3,\\ p\\in[0,1] \\\\\n\\Longrightarrow \\qquad \\ell(p|\\textbf{x}) &= [\\log(60) +]\\ 2\\log(p) + 3\\log(1 - p),\\ p\\in(0,1) \\\\\n\\Longrightarrow \\qquad \\frac{\\partial\\ell}{\\partial p} &= [0+]\\ \\frac{2}{p} - \\frac{3}{1 - p} \\\\\n\\Longrightarrow \\qquad 0 &\\overset{\\text{set}}{=} \\frac{2}{p} - \\frac{3}{1 - p} \\\\\n\\Longrightarrow \\qquad 0 &= 2(1 - p) - 3p \\\\\n\\Longrightarrow \\qquad \\hat{p} &= 2/5\n\\end{align}\\]\nThe second step is to confirm that \\(\\hat{p} = \\frac{2}{5}\\) is a maximum of \\(\\mathcal{L}\\), by ensuring that the second derivative of \\(\\mathcal{L}\\) is negative around \\(\\hat{p}\\). For that, we return to the first derivative of the log-likelihood (I’m using negative exponents instead of fractions because the chain rule is easier to apply than the quotient rule for these fractions), and differentiate again:\n\\[\\begin{align}\n\\frac{\\partial\\ell}{\\partial p} &= 2p^{-1} - 3(1 - p)^{-1} \\\\\n\\Longrightarrow \\qquad \\frac{\\partial^2\\ell}{\\partial p^2} &= -2p^{-2} + 3(1 - p)^{-2}\\times(-1) \\\\\n&= -\\left( \\frac{2}{p^2} + \\frac{3}{(1 - p)^2} \\right) &lt;0\\ \\forall p \\in (0,1).\n\\end{align}\\]\nSo, for this trivial example, both Method of Moments and Maximum Likelihood Estimation yielded the same estimate for \\(\\hat{p}\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  }
]