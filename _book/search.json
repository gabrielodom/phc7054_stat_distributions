[
  {
    "objectID": "chapters/bernoulli_20250310.html",
    "href": "chapters/bernoulli_20250310.html",
    "title": "2  The Bernoulli Distribution",
    "section": "",
    "text": "2.1 Deriving the Distribution\nIn the mid-1600s, mathematicians like Pascal and Fermat were obsessed with games of chance.1 The simplest such game is flipping a single coin. Let \\(P[A]\\) denote the probability of event \\(A\\) occurring. Because flipping a coin has only two outcomes (heads or tails; we ignore the microscopic possibility of a coin landing on its edge for practical gambling scenarios), we can define \\(p \\equiv P[\\text{head}]\\), which necessarily implies that \\(1 - p = P[\\text{tails}]\\). For ease of notation, we let \\(k\\in\\{0,1\\} = 1\\) when the coin hands on leads and \\(k = 0\\) for tails. Thus, we define a Bernoulli Trial as one random value drawn from the following distribution: \\[\nf_{\\text{Bern}}(k|p) = p^k(1-p)^{1-k},\\ k\\in\\{0,1\\},\\ p \\in (0,1) \\subset \\mathbb{R}.\n\\]\nNotice a few things:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#deriving-the-distribution",
    "href": "chapters/bernoulli_20250310.html#deriving-the-distribution",
    "title": "2  The Bernoulli Distribution",
    "section": "",
    "text": "The Bernoulli Probability Mass Function is denoted \\(f_{\\text{Bern}}\\); \\(f\\) is a function, and its argument \\(k\\) is discrete. The domain of \\(f\\) is 0 or 1 (\\(k\\) can only have the values in the set \\(\\mathcal{S} = \\{0,1\\}\\)).\nFor any \\(k \\in \\mathcal{S}\\), \\(f(k|p) \\ge 0\\); this is the range of \\(f\\). This means that \\(f\\) maps from the set \\(\\mathcal{S}\\) to the set of all non-negative real numbers, which is symbolically denoted as \\(f:\\mathcal{S} \\to \\mathbb{R}_{\\ge}\\).\nThe probability of a “head” (success) is the only parameter of \\(f\\), and it is fixed at some value \\(p\\), which must be a real number between 0 and 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#example-random-samples",
    "href": "chapters/bernoulli_20250310.html#example-random-samples",
    "title": "2  The Bernoulli Distribution",
    "section": "2.2 Example Random Samples",
    "text": "2.2 Example Random Samples\nWe now take some random samples from this distribution when \\(p = 0.5\\).\n\n\nCode\nset.seed(20150516)\n\nx &lt;- rbinom(n = 100, size = 1, prob = 0.5)\nsamples_ls &lt;- list(\n  n5   = x[1:5],\n  n15  = x[1:15],\n  n30  = x[1:30],\n  n100 = x\n)\n\nrm(x)\n\n\n\n\nCode\npar(mfrow = c(2, 2))\n\nhist(samples_ls$n5)\nhist(samples_ls$n15)\nhist(samples_ls$n30)\nhist(samples_ls$n100)\n\npar(mfrow = c(1, 1))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#show-that-this-is-a-distribution",
    "href": "chapters/bernoulli_20250310.html#show-that-this-is-a-distribution",
    "title": "2  The Bernoulli Distribution",
    "section": "2.3 Show that this is a Distribution",
    "text": "2.3 Show that this is a Distribution\n\n2.3.1 Formal Foundations: The Riemann-Stieltjes Integral\nLet \\(f\\) be a bounded function on the interval \\(\\mathcal{S} = [a, b] \\subset \\mathbb{R}\\), and let \\(G\\) be a monotone increasing (but not necessarily continuous) function on \\(\\mathcal{S}\\). The Riemann-Stieltjes integral of \\(f\\) with respect to \\(G\\) is denoted as \\[\n\\text{R-S}(f, G) \\equiv \\int_{x \\in \\mathcal{S}} f(x) \\text{d}G(x).\n\\] If \\(G\\) is continuous \\(\\forall x \\in \\mathcal{S}\\), then this integral simplifies to \\[\n\\int_{x \\in \\mathcal{S}} f(x) \\text{d}G(x) = \\int_{x \\in \\mathcal{S}} f(x) G^{\\prime}(x).\n\\] If, however, there exists \\(k &lt; m &lt; \\infty\\) points of discontinuity for \\(G\\) on \\(\\mathcal{S}\\), we define an \\(m\\)-partition of \\(\\mathcal{S}\\) as \\(\\{[y_0, y_1), [y_1, y_2), \\ldots, [y_{m - 2}, y_{m - 1}), [y_{m - 1}, y_m]\\}\\), where \\(\\{a = y_0, b = y_m\\}\\) and the \\(k\\) points of discontinuity are included in the sequence \\(\\{y_1, y_2, \\ldots, y_{m - 1}\\}\\). Then, this integral simplifies to \\[\n\\int_{x \\in \\mathcal{S}} f(x) \\text{d}G(x) = \\sum\\limits_{i = 1}^m f(x)\\left[ G(y_i) - G(y_{i - 1}) \\right].\n\\]\n\n\n2.3.2 Review: Properties of Distributions\nLet \\(x\\) be an observed value \\(\\in \\mathcal{A}\\), and let \\(\\boldsymbol\\theta\\) be a vector of parameters in a parameter space \\(\\boldsymbol\\Theta \\subseteq \\mathbb{R}^q\\). Consider a function \\(f(\\textbf{x}|\\boldsymbol\\theta)\\) with anti-derivative \\(F\\), and note that \\(f\\) need not be continuous. This \\(f\\) represents a probability distribution iff2\n\n\\(\\forall x \\in \\mathcal{S}\\), \\(\\forall \\boldsymbol\\theta \\in \\boldsymbol\\Theta\\), \\(f(x|\\boldsymbol\\theta) \\ge 0\\).\n\\(\\forall \\boldsymbol\\theta \\in \\boldsymbol\\Theta\\), \\(\\int_{x \\in \\mathcal{S}} \\text{d}F(x|\\boldsymbol\\theta) = 1\\), where \\(\\text{d}F\\) is the integrand of a Riemann-Stieltjes integral.\n\nAs long as (1) holds above, then \\(F\\) will be monotone increasing (because the anti-derivative of a non-negative function will always be flat or increasing). The probability density/mass functions for all statistical distributions share these two properties above. Because of the flexibility of the Riemann-Stieltjes integral, we don’t have to make the distinction between probability density functions and probability mass functions any longer. This is because\n\nIf \\(\\mathcal{S}\\) is a discrete set with cardinality \\(|\\mathcal{S}| = n\\), \\(f\\) is commonly referred to as a probability “mass” function. Then, because (1) holds, \\(\\exists\\) some ordering of the elements of \\(\\mathcal{S} \\ni 0 \\le F(x^{(1)}) \\le F(x^{(2)}) \\le \\cdots \\le F(x^{(n)}) \\le 1\\). We know that the total probability of all events is 1, and the total probability of no events is 0, so, by convention, we let \\(F(x^{(n)}) = 1\\) and \\(F(x^{(0)}) = 0\\). Thus, noticing the Telescoping Series3, \\[\n\\int_{x \\in \\mathcal{S}} \\text{d}F(x|\\boldsymbol\\theta) = \\sum\\limits_{i = 1}^n F(x^{(i)}) - F(x^{(i - 1)}) = F(x^{(n)}) - F(x^{(0)}) = 1.\n\\]\nIf \\(\\mathcal{S} = [a,b]\\) is a continuous set, then \\(f\\) is a probability “density” function. For this continuous range, \\(F(a) = 0\\) and \\(F(b) = 1\\). Thus, \\[\n\\int_{x \\in \\mathcal{S}} \\text{d}F(x|\\boldsymbol\\theta) = \\int_a^b F^{\\prime}(x) = F(b) - F(a) = 1.\n\\]\n\nThus, for the remainder of these notes, we will start all integral-based definitions with the Riemann-Stieltjes form, and then reduce this form into traditional sums or integrals, as is appropriate for the distribution at hand.\n\n\n2.3.3 Properties of the Bernoulli Distribution\nGiven the extensive review above, showing that \\(f_{\\text{Bern}}\\) is a probability distribution is anti-climactic.\nClaim 1: The function \\(f_{\\text{Bern}}\\) must be non-negative for all values of its support given \\(p\\) in the parameter space \\((0,1)\\).\nArgument 1: If \\(k = 0\\), then \\(f_{\\text{Bern}}(k = 0|p) = p^0(1 - p)^1 = 1 - p \\ge 0\\). Similarly, if \\(k = 1\\), then \\(f_{\\text{Bern}}(k = 1|p) = p^1(1 - p)^0 = p \\ge 0\\).\nClaim 2: The integral of the function \\(f_{\\text{Bern}}\\) over all possible values of \\(k\\) must be 1.\nArgument 2: Consider that \\[\n\\begin{aligned}\n\\int_{x \\in \\mathcal{S}} \\text{d}F(x|\\boldsymbol\\theta) &= \\sum\\limits_{k = 0}^1 f_{\\text{Bern}}(k|p) \\\\\n  &= \\left[ p^k(1-p)^{1-k} \\right]_{k = 0} + \\left[ p^k(1-p)^{1-k} \\right]_{k = 1} \\\\\n  &= [p^0(1-p)^1] + [p^1(1-p)^0] \\\\\n  &= (1 - p) + p \\\\\n  &= 1.\n\\end{aligned}\n\\]\n\n\n2.3.4 An Example Sample from the Bernoulli Distribution\nNow let’s use R to take \\(n = 100\\) random samples from a Bernoulli Distribution with \\(p = 0.35\\), but we will only inspect the first five (for now):\n\nmyBernSample &lt;- rbinom(n = 100, size = 1, prob = 0.35)\nmyBernSample[1:5]\n[1] 1 0 1 0 0\n\nWe now pretend that we only know the results for the first five coin flips. We have flipped one coin five times, with the results \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#derive-the-moment-generating-function",
    "href": "chapters/bernoulli_20250310.html#derive-the-moment-generating-function",
    "title": "2  The Bernoulli Distribution",
    "section": "2.4 Derive the Moment Generating Function",
    "text": "2.4 Derive the Moment Generating Function\n\n2.4.1 Review: What is the MGF?\nThe Moment Generating Function4 (MGF) is, as its name implies, a function to “generate” (i.e., calculate) moments. In statistics, I have not found great intuition on what a “moment” is, other than it relates to various measures of a probability distribution:\n\nThe 0\\(^{\\text{th}}\\) moment is the total area of the probability distribution [or 1],\nThe 1\\(^{\\text{st}}\\) moment is the expected value,\nThe 2\\(^{\\text{nd}}\\) (central) moment is the variance,\nThe 3\\(^{\\text{rd}}\\) moment is the skewness, and\nThe 4\\(^{\\text{th}}\\) moment is the kurtosis.\n\nPhysics has more intuition of moments, where the 1\\(^{\\text{st}}\\) moment is the center of mass for a body (the point at which you could balance the shape on a pencil) and the 2\\(^{\\text{nd}}\\) moment is the moment of inertia (how much mass is spread out away from the axis at the center of mass, where larger values mean the mass is spread out further away from the first moment).\nGiven a Cumulative Density Function \\(F_X(x|\\boldsymbol\\theta)\\), the MGF of \\(F_X\\) with respect to some value \\(t\\) in an \\(\\epsilon\\)-neighborhood5 of 0 is defined to be \\[\nM_X(t) \\equiv \\mathbb{E}\\left[ e^{tX} \\right] = \\int\\limits_{x \\in \\mathcal{S}(X)} e^{tx} \\text{d}F_X(x|\\boldsymbol\\theta),\n\\] where \\(\\text{d}F_X\\) is the integrand of a Riemann-Stieltjes integral (as discussed above).\nIf we have a distribution with \\(j\\) parameters, the process to calculate the first \\(j\\) moments is to take the first \\(j\\) derivatives of \\(M_X\\) and evaluate these functions (if they exist) at \\(t = 0\\). Then, these theoretical moments (functions of the distribution’s parameters \\(\\boldsymbol\\theta\\)) are set equal to the first \\(j = |\\boldsymbol\\theta|\\) sample moments, yielding a system of (often non-linear) equations to solve.\n\n\n2.4.2 MGF of the Bernoulli Distribution\nGiven the definition above, we can calculate the MGF: \\[\n\\begin{aligned}\nM_K(t) &\\equiv \\mathbb{E}\\left[ e^{tK} \\right] \\\\\n  &= \\int\\limits_{k \\in \\{0,1\\}} e^{tk} \\text{d}F_K(k|p) \\\\\n  &= \\sum\\limits_{k = 0}^1 e^{tk} p^k (1 - p)^{1 - k} \\\\\n  &= \\sum\\limits_{k = 0}^1 (pe^t)^k (1 - p)^{1 - k} \\\\\n  &= \\left[ (pe^t)^0 (1 - p)^1 \\right] + \\left[ (pe^t)^1 (1 - p)^0 \\right] \\\\\n  &= 1 - p + pe^t.\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#method-of-moments-estimators",
    "href": "chapters/bernoulli_20250310.html#method-of-moments-estimators",
    "title": "2  The Bernoulli Distribution",
    "section": "2.5 Method of Moments Estimators",
    "text": "2.5 Method of Moments Estimators\nNow that we have the MGF of the Bernoulli Distribution, we follow the process to calculate the theoretical moment(s) and set them equal to their corresponding sample moment(s). Because the Bernoulli Distribution only has \\(j = 1\\) parameter, our systems to solve will be somewhat trivial.\n\n2.5.1 First Moment\nGiven the MGF calculated above, we begin with \\[\n\\begin{aligned}\nM_K(t) &= 1 - p + pe^t \\\\\n\\Longrightarrow\\qquad \\frac{\\partial}{\\partial t}M_K(t) &= pe^t \\\\\n\\Longrightarrow\\qquad \\frac{\\partial}{\\partial t}M_K(0) &= p \\\\\n&= \\mathbb{E}[X].\n\\end{aligned}\n\\]\n\n\n2.5.2 The Second Moment\nThe second (non-central) moment is then \\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial t}M_K(t) &= pe^t \\\\ \\\\\n\\Longrightarrow\\qquad \\frac{\\partial^2}{\\partial t^2}M_K(t) &= pe^t \\\\\n\\Longrightarrow\\qquad \\frac{\\partial^2}{\\partial t^2}M_K(0) &= p \\\\\n&= \\mathbb{E}[X^2].\n\\end{aligned}\n\\]\nBecause this is the non-central moment, we find the second central moment by exploiting the common relationship between variance and moments; that is, \\[\n\\begin{aligned}\n\\text{Var}[X] &= \\mathbb{E}[X^2] - \\mathbb{E}^2[X] \\\\\n&= p - [p]^2 \\\\\n&= p(1 - p).\n\\end{aligned}\n\\]\n\n\n2.5.3 Solving the System\n\nFor distributions with two parameters, we would now equate these two population moments, \\(\\mathbb{E}[X]\\) and \\(\\text{Var}[X]\\), the two sample moments, \\(\\bar{x}\\) and \\(s^2\\), to yield the Method of Moments estimators for the two parameters of the distribution (which we will represent generically as \\(\\hat\\theta_{MoM}\\) and \\(\\hat\\phi_{MoM}\\)).\n\nHowever, the Bernoulli has only one parameter, so our “system” of equations is just \\[\n\\begin{aligned}\n\\bar{x} &= p \\\\\n\\Longrightarrow \\hat{p}_{MoM} &= \\bar{x}.\n\\end{aligned}\n\\] For the data observed above, the coin-flip results \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\) which came from a coin with \\(P[\\text{Heads}] = 0.35\\), \\(\\hat{p}_{MoM} = \\frac{1}{5}\\sum_{i = 1}^5 x_i = 0.4\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#maximum-likelihood-estimators",
    "href": "chapters/bernoulli_20250310.html#maximum-likelihood-estimators",
    "title": "2  The Bernoulli Distribution",
    "section": "2.6 Maximum Likelihood Estimators",
    "text": "2.6 Maximum Likelihood Estimators\nIn standard statistical inference, we assume that the population parameter is some fixed but unknown value and that our observed data are random; we must estimate the unknown (but fixed) parameter using statistics of the observed (but random) data. Likelihood functions (and the related Bayesian school of thought) turn this question “inside out”. The likelihood instead assumes that the parameter is a random variable (still unknown), but that the data are both observed and fixed.\nLet’s recall the observed toy data: we flipped one coin five times and observed \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\). Is the coin fair? Traditional statistics would say “\\(p\\) is unknown, but if I can collect enough data then I can estimate it with a narrow confidence interval”. In a simple coin-flip example, this is reasonable: we can hire a person to repeatedly flip that same coin for hours and hours and record the results. Eventually, we will have a sample size large enough so that we can build a confidence interval small enough to say with any requested level of “confidence” that the coin is or isn’t fair.\nRecall the \\((1-\\alpha)\\)-level confidence interval for a Bernoulli \\(p\\) is \\[\n\\text{CI}(p|n,\\alpha) =\n  \\hat{p} \\pm \\frac{z_{\\alpha/2}}{\\sqrt{n}}\\sqrt{\\hat{p}(1 - \\hat{p})},\n\\] where \\(z_{\\alpha/2}\\) is the Standard Normal \\(z\\) corresponding to the quantiles \\(1 - \\alpha/2\\) and \\(\\alpha/2\\). As we saw above, the true parameter value was \\(p = 0.35\\), which (we remark) would be impossible to know in real life.\nWe will assume that we take each coin flip one at a time. How many times will we have to flip the coin before the 95% CI no longer contains 0.5?\n\n\nCode\nzAlpha &lt;- qnorm(p = 0.975)\nresults_ls &lt;- lapply(\n  X = seq.int(from = 2, to = length(myBernSample), by = 1),\n  FUN = function(n_i) {\n    \n    x &lt;- myBernSample[1:n_i]\n    pHat &lt;- mean(x)\n    CIwidth &lt;- ( zAlpha / sqrt(n_i) ) * sqrt( pHat * (1 - pHat) )\n    \n    data.frame(\n      N = n_i,\n      pHat = pHat,\n      CIlb = pHat - CIwidth,\n      CIub = pHat + CIwidth\n    )\n  }\n)\n\nresults_df &lt;- do.call(rbind, results_ls)\n\n# add trivial CI results for first coin flip\nresults_df &lt;- rbind(\n  data.frame(\n    N = 1, pHat = myBernSample[1],\n    CIlb = myBernSample[1], CIub = myBernSample[1]\n  ),\n  results_df\n)\n\n\nLet’s plot these results:\n\n\nCode\nlibrary(ggplot2)\nggplot(data = results_df[1:100, ]) + \n  aes(x = N) +\n  ylim(c(0, 1)) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 10)) +\n  geom_abline(slope = 0, intercept = 0.5, colour = \"red\") + \n  geom_line(aes(y = CIlb)) + \n  geom_line(aes(y = CIub))\n\n\n\n\n\n\n\n\n\nAs we can see, we need between 85-90 coin flips before we can be 95% sure that the coin is “not fair” (that the true value of \\(p\\ne 0.5\\)). Also, we see that the confidence intervals are degenerate for a few of the samples with \\(n\\le 5\\) (that the bounds of the confidence interval for \\(p\\) are outside \\([0,1]\\), which is impossible).\nFor the computer, flipping additional coins to create new data is trivially inexpensive. However, in real life, collecting one additional sample may be of enormous cost to the research team. In these cases, it doesn’t help us that we would theoretically be able to reject the claim that \\(p = 0.5\\) at some point in the future (with more samples), we need to be able to make a statement about the likelihood that the coin is fair with the samples we have right now. Let the event \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\) be encoded \\(\\textbf{x} = (1,0,1,0,0)\\). Thus, we apply the multiplication rule of independent events for these five coin flips, and define the likelihood function of \\(p\\) given the observed data:\n\\[\\begin{align}\n\\mathcal{L}(p|\\textbf{x}) &=\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 1} \\times\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 0} \\times \\\\\n  &\\qquad \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 1} \\times\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 0} \\times\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 0} \\\\\n  \n  &= \\left[ p^1 \\right] \\times \\left[ (1 - p)^1 \\right] \\times\n  \\left[ p^1 \\right] \\times \\left[ (1 - p)^1 \\right] \\times \\left[ (1 - p)^1 \\right] \\\\\n  &= p^2(1 - p)^3 \\\\\n  &= p^2(1 - 3p + 3p^2 - p^3) \\\\\n  &= p^2 - 3p^3 + 3p^4 - p^5.\n\\end{align}\\]\nThis function \\(\\mathcal{L}\\) contains almost all the information we have about \\(p\\): it has all the data, and it has our best guess about the data generating process (we think it’s a Bernoulli trial). A few things to notice:\n\n\\(\\mathcal{L}\\) is a function of the parameter, \\(p\\), not of the data.\n\\(\\mathcal{L}\\) is NOT a probability function; even though \\(\\mathcal{L} \\ge 0\\ \\forall p\\), we have that the integral over the support of \\(p\\) is\n\n\\[\\begin{align}\n\\int_0^1 \\mathcal{L}(p|\\textbf{x})dp &=\n  \\int_0^1 \\left[ p^2 - 3p^3 + 3p^4 - p^5 \\right]dp \\\\\n  &= \\frac{1}{3}p^3 - \\frac{3}{4}p^4 + \\frac{3}{5}p^5 - \\frac{1}{6}p^6 \\Big\\rvert_0^1 \\\\\n  &= \\frac{1}{3} - \\frac{3}{4} + \\frac{3}{5} - \\frac{1}{6} \\\\\n  &= \\frac{20}{60} - \\frac{45}{60} + \\frac{36}{60} - \\frac{10}{60} \\\\\n  &= \\frac{1}{60} \\\\\n  &\\ne 1\n\\end{align}\\]\nThis exercise serves two purposes: first to show that \\(\\mathcal{L}\\) is not a probability distribution, and second to show the value of the multiplicative constant which would make a distribution. That is, \\(f(p|\\textbf{x}) = 60*\\mathcal{L}(p|\\textbf{x})\\) is a probability distribution. Now we can make probabilistic statements about the value of \\(p\\).\nWe can see what the probability distribution function looks like:\n\n\nCode\np &lt;- seq(from = 0, to = 1, length.out = 101)\nf_p &lt;- 60 * p^2 * (1 - p)^3\n\nplot(x = p, y = f_p)\n\n\n\n\n\n\n\n\n\nAnd since we have calculated the indefinite integral already, we can plot the Cumulative Distribution Function:\n\n\nCode\nF_p &lt;- 60 * (p^3/3 - 3*p^4/4 + 3*p^5/5 - p^6/6)\nplot(x = p, y = F_p)\n\n\n\n\n\n\n\n\n\nFinally, we can make statements about the claim that \\(p = 0.5\\). For instance, what is \\(P[p &lt; 0.5]\\)?\n\nF_p[which(p == 0.5)]\n[1] 0.656\n\nWhat is \\(P[p &gt; 0.5]\\)?\n\n1 - F_p[which(p == 0.5)]\n[1] 0.344\n\nWhat is \\(P[0.4 &lt; p &lt; 0.6]\\)?\n\nF_p[which(p == 0.6)] - F_p[which(p == 0.4)]\n[1] 0.365\n\nWhat is an 80% credible set6 for \\(p\\)?\n\n# Lower\np[max(which(F_p &lt; 0.1))]\n[1] 0.2\n# Upper\np[min(which(F_p &gt; 0.9))]\n[1] 0.67",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#exercises",
    "href": "chapters/bernoulli_20250310.html#exercises",
    "title": "2  The Bernoulli Distribution",
    "section": "2.8 Exercises",
    "text": "2.8 Exercises\nTo be determined.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#footnotes",
    "href": "chapters/bernoulli_20250310.html#footnotes",
    "title": "2  The Bernoulli Distribution",
    "section": "",
    "text": "https://www.usu.edu/math/schneit/StatsStuff/Probability/probability2.html↩︎\nif and only if↩︎\n(https://en.wikipedia.org/wiki/Telescoping_series)↩︎\nhttps://en.wikipedia.org/wiki/Moment-generating_function↩︎\n\\(t \\in (-\\epsilon, \\epsilon) \\subset \\mathbb{R}\\) (where \\(\\epsilon\\) is an arbitrarily small value)↩︎\nWe could also notice that this is the kernel of a Beta distribution with \\(\\alpha = 3\\) and \\(\\beta = 4\\), with normalizing constant \\(\\frac{(3-1)!(4-1)!}{(3+4-1)!} = \\frac{2}{6*5*4} = \\frac{1}{60}\\), but that would make the next steps too easy…↩︎\nhttps://en.wikipedia.org/wiki/Credible_interval↩︎\nhttps://tutorial.math.lamar.edu/classes/calci/shapeofgraphptii.aspx↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html",
    "href": "chapters/binomial_20250310.html",
    "title": "3  The Binomial Distribution",
    "section": "",
    "text": "3.1 Deriving the Distribution\nIn the previous chapter, we explored the properties of a Bernoulli trial. We envisioned a scenario where a person flipped a coin five times, and the results were \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\). What we knew, but the hypothetical person did no know, was that this particular coin was not “fair”. In fact, these five observations were drawn from a Bernoulli process with \\(P[\\text{Heads}] = 0.35\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#example-random-samples",
    "href": "chapters/binomial_20250310.html#example-random-samples",
    "title": "3  The Binomial Distribution",
    "section": "3.2 Example Random Samples",
    "text": "3.2 Example Random Samples\n\n\nCode\nset.seed(20150516)\n\nN &lt;- 10\nbins_int &lt;- seq.int(from = -1, to = N, by = 1)\n\nxSymm &lt;- rbinom(n = 100, size = N, prob = 0.5)\nsamplesSymm_ls &lt;- list(\n  n5   = xSymm[1:5],\n  n15  = xSymm[1:15],\n  n30  = xSymm[1:30],\n  n100 = xSymm\n)\n\nxSkew &lt;- rbinom(n = 100, size = N, prob = 0.2)\nsamplesSkew_ls &lt;- list(\n  n5   = xSkew[1:5],\n  n15  = xSkew[1:15],\n  n30  = xSkew[1:30],\n  n100 = xSkew\n)\n\nrm(xSymm, xSkew)\n\n\n\n\nCode\npar(mfrow = c(2, 2))\n\nhist(samplesSymm_ls$n5, breaks = bins_int)\nhist(samplesSymm_ls$n15, breaks = bins_int)\nhist(samplesSymm_ls$n30, breaks = bins_int)\nhist(samplesSymm_ls$n100, breaks = bins_int)\n\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 2))\n\nhist(samplesSkew_ls$n5, breaks = bins_int)\nhist(samplesSkew_ls$n15, breaks = bins_int)\nhist(samplesSkew_ls$n30, breaks = bins_int)\nhist(samplesSkew_ls$n100, breaks = bins_int)\n\npar(mfrow = c(1, 1))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#show-that-this-is-a-distribution",
    "href": "chapters/binomial_20250310.html#show-that-this-is-a-distribution",
    "title": "3  The Binomial Distribution",
    "section": "3.3 Show that this is a Distribution",
    "text": "3.3 Show that this is a Distribution\nLet \\(\\mathcal{S} = \\mathbb{N} \\cup 0\\), where \\(\\mathbb{N}\\) denotes the set of natural numbers.3 Let \\(f(k|n,p) = {n \\choose k} p^{k}(1 - p)^{n - k}\\) represent the probability function of the Binomial Distribution. We must now show that\n\n\\(\\forall k \\in \\mathcal{S}\\), and for \\(p \\in (0,1)\\), \\(f(k|n, p) \\ge 0\\), and\nfor \\(p \\in (0,1)\\), \\(\\int_{k \\in \\mathcal{S}} \\text{d}F(k|n,p) = 1\\).\n\n\n3.3.1 Formal Foundations: Pascal’s Rule\nFor writing proofs involving combinations, we need the following property, known as Pascal’s Formula,4 which states that\n\\[\n{a \\choose b} = {a - 1 \\choose b} + {a - 1 \\choose b - 1}.\n\\]\nProof: For integers \\(a,b\\), we construct this identity directly via simplification: \\[\n\\begin{aligned}\n{a - 1 \\choose b} + {a - 1 \\choose b - 1} &=\n  \\frac{(a - 1)!}{b!(a - b - 1)!} + \\frac{(a - 1)!}{(b - 1)!(a - 1 - b + 1)!} \\\\\n  &= \\frac{a - b}{a - b}\\left[ \\frac{(a - 1)!}{b!(a - b - 1)!} \\right] + \\frac{b}{b} \\left[ \\frac{(a - 1)!}{(b - 1)!(a - b)!} \\right] \\\\\n  &= \\frac{a(a - 1)! - b(a - 1)!}{b!(a - b)(a - b - 1)!} + \\frac{b(a - 1)!}{b(b - 1)!(a - b)!} \\\\\n  &= \\frac{a! - b(a - 1)!}{b!(a - b)!} + \\frac{b(a - 1)!}{b!(a - b)!} \\\\\n  &= \\frac{a!}{b!(a - b)!} \\\\\n  &\\equiv {a \\choose b}.\n\\end{aligned}\n\\]\n\n\n3.3.2 Formal Foundations: Mathematical Induction\nThe next foundational proof requires a technique called “Proof by Induction”, or more generally, mathematical induction.5 This is the structure of such a proof:\n\nProof by Induction: Consider a sequence of equations indexed over the integers by \\(n\\). To show that the sequence of equations is true \\(\\forall n\\), we\n\nshow that the equation is true for \\(n = 1\\) [the “base case”],\nassume that the equation is true for \\(n = i\\) [the “hypothesis”], then\nprove that the equation is true for \\(n = i + 1\\) from the case when \\(n = i\\) [the “induction”].\n\n\nHere is a trivial example. Let’s prove \\(\\forall k \\in\\mathbb{N} \\ge 5\\) that \\(2^k &lt; \\Gamma(k + 1)\\) (the point of this proof is to show that the factorial function increases more rapidly to \\(\\infty\\) than the exponential function). Our base case is for \\(k = 5\\). We know that \\(2^5 = 32\\) and \\(\\Gamma(k + 1) = k! = 5! = 120\\). Because \\(32 &lt; 120\\), the base case is true. Our hypothesis, what we assume to be true, is that \\(2^i &lt; \\Gamma(i + 1)\\). To logically induct, we assume our hypothesis is true, and then show that our hypothesis implies that \\(2^{i + 1} &lt; \\Gamma(i + 2)\\). That is \\[\n\\begin{aligned}\n2^{i+1} &\\overset{?}{&lt;} \\Gamma(i + 2) \\\\\n\\Longrightarrow 2 \\times 2^i &\\overset{?}{&lt;} i \\times \\Gamma(i + 1) \\\\\n\\Longrightarrow \\frac{2}{i} \\times 2^i &&lt; \\Gamma(i + 1),\n\\end{aligned}\n\\] which is true for \\(i \\ge 5\\) because our hypothesis was that \\(2^i &lt; \\Gamma(i + 1)\\). Thus, \\(2^k &lt; \\Gamma(k + 1)\\ \\forall k \\in\\mathbb{N} \\ge 5\\), which completes our proof.\n\n\n3.3.3 Formal Foundations: Mathematical Induction Proof of the Binomial Theorem\nFor this section, we will also need to use the Binomial Theorem:6\n\\[\n(x +  y)^n = \\sum_{k = 0}^n {n \\choose k} x^k y^{n - k}.\n\\]\nBefore we can use this, we must prove that it is true.\n\n3.3.3.1 The Base Case\nLet \\(n = 1\\). Then\n\\[\n\\begin{aligned}\n\\sum_{k = 0}^1 {1 \\choose k} x^k y^{1 - k} &=\n  \\left[ {1 \\choose 0} x^0 y^{1 - 0} \\right] + \\left[ {1 \\choose 1} x^1 y^{1 - 1} \\right] \\\\\n  &= \\frac{1!}{0!\\times 1!} (1) y^1 + \\frac{1!}{1!\\times 0!} x^1(1) \\\\\n  &= y + x \\\\\n  &= (x + y)^1.\n\\end{aligned}\n\\]\n\n\n3.3.3.2 The Hypothesis\nWe assume that this equation is true for \\(n = i\\). That is, we assume that\n\\[\n\\sum_{k = 0}^i {i \\choose k} x^k y^{i - k} = (x + y)^i.\n\\]\n\n\n3.3.3.3 The Induction\nAssuming that the hypothesis for \\(n = i\\) is true, we will show that the equation also holds for \\(n = i + 1\\). Note that the end of the proof requires Pascal’s Rule to combine sums of combinations, and we comment that there is only one way to “choose” 0 things or all things. Thus,\n\\[\n\\begin{aligned}\n(x + y)^i &= \\sum_{k = 0}^i {i \\choose k} x^k y^{i - k} \\\\\n\\Longrightarrow (x + y)^{i + 1} &= (x + y)\\sum_{k = 0}^i {i \\choose k} x^k y^{i - k} \\\\\n  &= (x + y)\\left[ {i \\choose 0} x^0 y^{i - 0} + {i \\choose 1} x^1 y^{i - 1} + \\ldots + {i \\choose i - 1} x^{i - 1} y^1 + {i \\choose i} x^{i - 0} y^0 \\right] \\\\\n  &= x\\left[ {i \\choose 0} x^0 y^i + {i \\choose 1} x^1 y^{i - 1} + \\ldots + {i \\choose i - 1} x^{i - 1} y^1 + {i \\choose i} x^i y^0 \\right] + \\\\\n  &\\quad\\ \\  y\\left[ {i \\choose 0} x^0 y^i + {i \\choose 1} x^1 y^{i - 1} + \\ldots + {i \\choose i - 1} x^{i - 1} y^1 + {i \\choose i} x^i y^0 \\right] \\\\\n  &= \\left[ {i \\choose 0} x^1 y^i + {i \\choose 1} x^2 y^{i - 1} + \\ldots + {i \\choose i - 1} x^i y^1 + {i \\choose i} x^{i+1} y^0 \\right] + \\\\\n  &\\quad\\  \\left[ {i \\choose 0} x^0 y^{i + 1} + {i \\choose 1} x^1 y^i + \\ldots + {i \\choose i - 1} x^{i - 1} y^2 + {i \\choose i} x^i y^1 \\right] \\\\\n  \\\\[0.1mm]\n  &\\qquad \\text{\\emph{Collect like terms...}} \\\\\n  &= {i \\choose 0} x^0 y^{i + 1} + \\\\\n  &\\qquad \\left[ {i \\choose 0} x^1 y^i + {i \\choose 1} x^1 y^i\\right] + \\ldots + \\left[ {i \\choose i - 1} x^i y^1 + {i \\choose i} x^i y^1 \\right] + \\\\\n  &\\qquad {i \\choose i} x^{i+1} y^0 \\\\\n  \\\\[0.1mm]\n  &\\qquad \\text{\\emph{Only one way to choose all or none...}} \\\\\n  &= (1) x^0 y^{i + 1} + \\\\\n  &\\qquad \\left[ {i \\choose 0} + {i \\choose 1} \\right]x^1 y^i + \\ldots + \\left[ {i \\choose i - 1} + {i \\choose i} \\right]x^i y^1 + \\\\\n  &\\qquad (1) x^{i+1} y^0 \\\\\n  \\\\[0.1mm]\n  &\\qquad \\text{\\emph{Pascal's Rule...}} \\\\\n  &= (1) x^0 y^{i + 1} + \\\\\n  &\\qquad \\left[ {i + 1 \\choose 1} \\right]x^1 y^i + \\ldots + \\left[ {i + 1 \\choose i} \\right]x^i y^1 + \\\\\n  &\\qquad (1) x^{i+1} y^0 \\\\\n  &= {i + 1 \\choose 0} x^0 y^{i + 1} + {i + 1 \\choose 1}x^1 y^i + \\ldots + {i + 1 \\choose i}x^i y^1 + {i + 1 \\choose i + 1} x^{i+1} y^0 \\\\\n  \\\\[0.1mm]\n  &\\qquad \\text{\\emph{By definition...}} \\\\\n  &= \\sum_{k = 0}^{i + 1} {i + 1 \\choose k} x^k y^{i + 1 - k}.\n\\end{aligned}\n\\]\nTherefore, \\(\\forall n \\in \\mathbb{N}\\), \\[\n\\sum_{k = 0}^n {n \\choose k} x^k y^{n - k} = (x + y)^n.\n\\]\n\n\n\n3.3.4 The Distribution is Non-negative\nConsider \\(f\\) defined above. First, we notice that the Binomial Coefficient is defined as a ratio of factorials; the standard definition of factorials only includes the natural numbers (\\(\\mathbb{N}\\)), so they are necessarily positive. The ratio of two positive numbers is positive. Second, we have that \\(k,\\ n - k \\ge 0\\), and that \\(p &gt; 0\\). Non-negative powers of positive numbers are also positive. Setting \\(p = 0\\) yields a degenerate distribution anyway, so we don’t bother with it. Putting these together for \\(p \\in (0,1)\\), we have that \\(f = 0\\) outside the support of \\(k\\) and \\(f &gt; 0\\) for \\(k \\le n, \\ni \\{k,\\ n\\} \\in \\mathcal{S}\\).\n\n\n3.3.5 The Total Probability is 1\nRecall the Binomial Theorem we proved above, and let \\(x = p\\) and \\(y = 1 - p\\). Then, \\[\n\\begin{aligned}\n\\int_{k \\in \\mathcal{S}} \\text{d}F(k|n,p) &= \\sum_{k = 0}^n {n \\choose k} p^{k}(1 - p)^{n - k} \\\\\n  &= \\left[ p + (1 - p) \\right]^n \\\\\n  &= 1^n \\\\\n  &= 1.\n\\end{aligned}\n\\]\nTherefore, because the function \\(f(k|n,p)\\) is always non-negative and it’s Riemann-Stieljes integral is 1, the Binomial Distribution is a true distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#derive-the-moment-generating-function",
    "href": "chapters/binomial_20250310.html#derive-the-moment-generating-function",
    "title": "3  The Binomial Distribution",
    "section": "3.4 Derive the Moment Generating Function",
    "text": "3.4 Derive the Moment Generating Function\nDeriving the MGF for the Binomial is a straightforward application of the definition and Binomial Theorem. That is, \\[\n\\begin{aligned}\nM_k(t) &\\equiv \\mathbb{E}\\left[ e^{tX} \\right] \\\\\n  &= \\int\\limits_{k \\in \\mathcal{S}(K)} e^{tk} \\text{d}F_K(k|n,p) \\\\\n  &= \\sum\\limits_{k = 0}^n e^{tk} {n \\choose k} p^{k}(1 - p)^{n - k} \\\\\n  &= \\sum\\limits_{k = 0}^n {n \\choose k} \\left[e^t\\right]^k p^{k}(1 - p)^{n - k} \\\\\n  &= \\sum\\limits_{k = 0}^n {n \\choose k} \\left[e^t\\right]^k p^{k}(1 - p)^{n - k} \\\\\n  &= \\sum\\limits_{k = 0}^n {n \\choose k} \\left[e^tp\\right]^k (1 - p)^{n - k} \\\\\n  &= \\left[ e^tp + (1 - p) \\right]^n.\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#method-of-moments-estimators",
    "href": "chapters/binomial_20250310.html#method-of-moments-estimators",
    "title": "3  The Binomial Distribution",
    "section": "3.5 Method of Moments Estimators",
    "text": "3.5 Method of Moments Estimators\nNow that we have this MGF, we can find the Method of Moments (MoM) estimator for \\(p\\), and we will comment on such an estimator for \\(n\\) when it is unknown.\n\n3.5.1 The First Moment\nWe have that \\[\n\\begin{aligned}\nM_k(t) &= \\left[ e^tp + (1 - p) \\right]^n \\\\\n\\Longrightarrow M^{\\prime}_k(t) &= \\frac{\\partial}{\\partial t} \\left[ e^tp + (1 - p) \\right]^n \\\\\n  &= n \\left[ e^tp + (1 - p) \\right]^{n - 1} \\frac{\\partial}{\\partial t} \\left[ e^tp + (1 - p) \\right] \\\\\n  &= n \\left[ e^tp + (1 - p) \\right]^{n - 1} e^tp \\\\\n\\Longrightarrow M^{\\prime}_k(0) &= n \\left[ (1)p + (1 - p) \\right]^{n - 1} (1)p \\\\\n  &= n(1)^{n - 1}p \\\\\n  &= np.\n\\end{aligned}\n\\] Therefore, \\(\\mathbb{E}[k] = np\\).\n\n\n3.5.2 The Second Non-Central Moment\nTaking the second derivative with respect to \\(t\\), we have \\[\n\\begin{aligned}\nM^{\\prime}_k(t) &= ne^tp \\left[ e^tp + (1 - p) \\right]^{n - 1} \\\\\n\\Longrightarrow M^{\\prime\\prime}_k(t) &= ne^tp \\frac{\\partial}{\\partial t} \\left[ e^tp + (1 - p) \\right]^{n - 1} + \\left[ e^tp + (1 - p) \\right]^{n - 1} \\frac{\\partial}{\\partial t} ne^tp \\\\\n  &= ne^tp \\times (n - 1) \\left[ e^tp + (1 - p) \\right]^{n - 2} e^tp + \\left[ e^tp + (1 - p) \\right]^{n - 1} ne^tp \\\\\n\\Longrightarrow M^{\\prime\\prime}_k(0) &= n(1)p \\times (n - 1) \\left[ (1)p + (1 - p) \\right]^{n - 2} (1)p + \\left[ (1)p + (1 - p) \\right]^{n - 1} n(1)p \\\\\n  &= np^2 (n - 1) (1)^{n - 2} + np (1)^{n - 1} \\\\\n  &= np \\left[(n - 1)p + 1\\right].\n\\end{aligned}\n\\] Therefore, \\(\\mathbb{E}[k^2] = np(np - p + 1)\\).\n\n\n3.5.3 The Second Central Moment\nThus \\[\n\\begin{aligned}\n\\text{Var}[k] &= \\mathbb{E}[k^2] - \\left(\\mathbb{E}[k]\\right)^2 \\\\\n  &= np(np - p + 1) - \\left( np \\right)^2 \\\\\n  &= np \\left[ np - p + 1 - np \\right] \\\\\n  &= np (1 - p).\n\\end{aligned}\n\\]\n\nTechnically, we don’t need the second central moment to find the Method of Moments estimators, but it is good practice.\n\n\n\n3.5.4 Solving the System of Equations\nNow that we have the first two population moments, \\(\\mathbb{E}[k]\\) and \\(\\text{Var}[k]\\), we can set them equal to the first two sample moments. That is, we solve the system \\[\n\\left\\{ \\mathbb{E}[k] = \\frac{1}{N} \\sum_{i = 1}^N k_i;\\ \\mathbb{E}[k^2] = \\frac{1}{N} \\sum_{i = 1}^N k_i^2 \\right\\},\n\\] where \\(N\\) is the number of Bernoulli trials in each Binomial experiment and \\(k_i\\) is the number of successes out of \\(N\\) attempts in Binomial experiment \\(i\\). For our example data, we were discussing the first such experiment, with observed data \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\); so we are on Binomial experiment \\(i\\), the number of coin flips was \\(N = 5\\), and we observed \\(k_i = 2\\) heads.\nGiven repeated \\(n\\) Binomial experiments of \\(N\\) Bernoulli trials each, we then solve the following system of equations: \\[\n\\left\\{ np = \\frac{1}{N} \\sum_{i = 1}^N k_i;\\ np(np - p + 1) = \\frac{1}{N} \\sum_{i = 1}^N k_i^2 \\right\\}.\n\\] Very often, \\(n\\) is known, so this simplifies to solving \\(\\hat{p}_{MoM} = \\frac{1}{nN} \\sum_{i = 1}^N k_i\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#maximum-likelihood-estimators",
    "href": "chapters/binomial_20250310.html#maximum-likelihood-estimators",
    "title": "3  The Binomial Distribution",
    "section": "3.7 Maximum Likelihood Estimators",
    "text": "3.7 Maximum Likelihood Estimators",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#exercises",
    "href": "chapters/binomial_20250310.html#exercises",
    "title": "3  The Binomial Distribution",
    "section": "3.8 Exercises",
    "text": "3.8 Exercises",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#the-likelihood-function",
    "href": "chapters/bernoulli_20250310.html#the-likelihood-function",
    "title": "2  The Bernoulli Distribution",
    "section": "2.6 The Likelihood Function",
    "text": "2.6 The Likelihood Function\nIn standard statistical inference, we assume that the population parameter is some fixed but unknown value and that our observed data are random; we must estimate the unknown (but fixed) parameter using statistics of the observed (but random) data. Likelihood functions (and the related Bayesian school of thought) turn this question “inside out”. The likelihood instead assumes that the parameter is a random variable (still unknown), but that the data are both observed and fixed.\n\n2.6.1 Reviewing the Repeated Sampling Paradigm\nLet’s recall the observed toy data: we flipped one coin five times and observed \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\). Is the coin fair? Traditional statistics would say “\\(p\\) is unknown, but if I can collect enough data then I can estimate it with a narrow confidence interval”. In a simple coin-flip example, this is reasonable: we can hire a person to repeatedly flip that same coin for hours and hours and record the results. Eventually, we will have a sample size large enough so that we can build a confidence interval small enough to say with any requested level of “confidence” that the coin is or isn’t fair.\nRecall the \\((1-\\alpha)\\)-level confidence interval for a Bernoulli \\(p\\) is \\[\n\\text{CI}(p|n,\\alpha) =\n  \\hat{p} \\pm \\frac{z_{\\alpha/2}}{\\sqrt{n}}\\sqrt{\\hat{p}(1 - \\hat{p})},\n\\] where \\(z_{\\alpha/2}\\) is the Standard Normal \\(z\\) corresponding to the quantiles \\(1 - \\alpha/2\\) and \\(\\alpha/2\\). As we saw above, the true parameter value was \\(p = 0.35\\), which (we remark) would be impossible to know in real life.\nWe will assume that we take each coin flip one at a time. How many times will we have to flip the coin before the 95% CI no longer contains 0.5?\n\n\nCode\nzAlpha &lt;- qnorm(p = 0.975)\nresults_ls &lt;- lapply(\n  X = seq.int(from = 2, to = length(myBernSample), by = 1),\n  FUN = function(n_i) {\n    \n    x &lt;- myBernSample[1:n_i]\n    pHat &lt;- mean(x)\n    CIwidth &lt;- ( zAlpha / sqrt(n_i) ) * sqrt( pHat * (1 - pHat) )\n    \n    data.frame(\n      N = n_i,\n      pHat = pHat,\n      CIlb = pHat - CIwidth,\n      CIub = pHat + CIwidth\n    )\n  }\n)\n\nresults_df &lt;- do.call(rbind, results_ls)\n\n# add trivial CI results for first coin flip\nresults_df &lt;- rbind(\n  data.frame(\n    N = 1, pHat = myBernSample[1],\n    CIlb = myBernSample[1], CIub = myBernSample[1]\n  ),\n  results_df\n)\n\n\nLet’s plot these results:\n\n\nCode\nlibrary(ggplot2)\nggplot(data = results_df) + \n  aes(x = N) +\n  ylim(c(0, 1)) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 10)) +\n  geom_abline(slope = 0, intercept = 0.5, colour = \"red\") + \n  geom_line(aes(y = CIlb)) + \n  geom_line(aes(y = CIub))\n\n\n\n\n\n\n\n\n\nAs we can see, we need between 85-90 coin flips before we can be 95% sure that the coin is “not fair” (that the true value of \\(p\\ne 0.5\\)). Also, we see that the confidence intervals are degenerate for a few of the samples with \\(n\\le 5\\) (that the bounds of the confidence interval for \\(p\\) are outside \\([0,1]\\), which is impossible).\n\n\n2.6.2 Making the Most Use of our Data\nFor the computer, flipping additional coins to create new data is trivially inexpensive. However, in real life, collecting one additional sample may be of enormous cost to the research team. In these cases, it doesn’t help us that we would theoretically be able to reject the claim that \\(p = 0.5\\) at some point in the future (with more samples), we need to be able to make a statement about the likelihood that the coin is fair with the samples we have right now. Let the event \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\) be encoded \\(\\textbf{x} = (1,0,1,0,0)\\). Thus, we apply the multiplication rule of independent events for these five coin flips, and define the likelihood function of \\(p\\) given the observed data:\n\\[\\begin{align}\n\\mathcal{L}(p|\\textbf{x}) &=\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 1} \\times\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 0} \\times \\\\\n  &\\qquad \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 1} \\times\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 0} \\times\n  \\left[ p^k(1 - p)^{1 - k} \\right]_{k = 0} \\\\\n  \n  &= \\left[ p^1 \\right] \\times \\left[ (1 - p)^1 \\right] \\times\n  \\left[ p^1 \\right] \\times \\left[ (1 - p)^1 \\right] \\times \\left[ (1 - p)^1 \\right] \\\\\n  &= p^2(1 - p)^3 \\\\\n  &= p^2(1 - 3p + 3p^2 - p^3) \\\\\n  &= p^2 - 3p^3 + 3p^4 - p^5.\n\\end{align}\\]\n\n\n2.6.3 Integrating the Likelihood\nThis function \\(\\mathcal{L}\\) contains almost all the information we have about \\(p\\): it has all the data, and it has our best guess about the data generating process (we think it’s a Bernoulli trial). A few things to notice:6\n\n\\(\\mathcal{L}\\) is a function of the parameter, \\(p\\), not of the data.\n\\(\\mathcal{L}\\) is NOT a probability function; even though \\(\\mathcal{L} \\ge 0\\ \\forall p\\), we have that the integral over the support of \\(p\\) is\n\n\\[\\begin{align}\n\\int_0^1 \\mathcal{L}(p|\\textbf{x})dp &=\n  \\int_0^1 \\left[ p^2 - 3p^3 + 3p^4 - p^5 \\right]dp \\\\\n  &= \\frac{1}{3}p^3 - \\frac{3}{4}p^4 + \\frac{3}{5}p^5 - \\frac{1}{6}p^6 \\Big\\rvert_0^1 \\\\\n  &= \\frac{1}{3} - \\frac{3}{4} + \\frac{3}{5} - \\frac{1}{6} \\\\\n  &= \\frac{20}{60} - \\frac{45}{60} + \\frac{36}{60} - \\frac{10}{60} \\\\\n  &= \\frac{1}{60} \\\\\n  &\\ne 1\n\\end{align}\\]\nThis exercise serves two purposes: first to show that \\(\\mathcal{L}\\) is not a probability distribution, and second to show the value of the multiplicative constant which would make a distribution. That is, \\(f(p|\\textbf{x}) = 60*\\mathcal{L}(p|\\textbf{x})\\) is a probability distribution. Now we can make probabilistic statements about the value of \\(p\\).\nWe can see what the probability distribution function looks like (it should look like a Beta distribution, because it is):\n\n\nCode\np &lt;- seq(from = 0, to = 1, length.out = 101)\nf_p &lt;- 60 * p^2 * (1 - p)^3\n\nplot(x = p, y = f_p)\n\n\n\n\n\n\n\n\n\nAnd since we have calculated the indefinite integral already, we can plot the Cumulative Distribution Function:\n\n\nCode\nF_p &lt;- 60 * (p^3/3 - 3*p^4/4 + 3*p^5/5 - p^6/6)\nplot(x = p, y = F_p)\n\n\n\n\n\n\n\n\n\nFinally, we can make statements about the claim that \\(p = 0.5\\). For instance, what is \\(P[p &lt; 0.5]\\)?\n\nF_p[which(p == 0.5)]\n[1] 0.656\n\nWhat is \\(P[p &gt; 0.5]\\)?\n\n1 - F_p[which(p == 0.5)]\n[1] 0.344\n\nWhat is \\(P[0.4 &lt; p &lt; 0.6]\\)?\n\nF_p[which(p == 0.6)] - F_p[which(p == 0.4)]\n[1] 0.365\n\nWhat is an 80% credible set7 for \\(p\\)?\n\n# Lower\np[max(which(F_p &lt; 0.1))]\n[1] 0.2\n# Upper\np[min(which(F_p &gt; 0.9))]\n[1] 0.67",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/bernoulli_20250310.html#maximum-likelihood-estimators-the-most-likely-value-of-p",
    "href": "chapters/bernoulli_20250310.html#maximum-likelihood-estimators-the-most-likely-value-of-p",
    "title": "2  The Bernoulli Distribution",
    "section": "2.7 Maximum Likelihood Estimators: The “Most Likely” Value of \\(p\\)",
    "text": "2.7 Maximum Likelihood Estimators: The “Most Likely” Value of \\(p\\)\nWe have an integrated likelihood, which contains basically almost all there is to know about the data we’ve collected, but finding a closed form of the integral of \\(\\mathcal{L}\\) (necessary to find \\(f\\) and \\(F\\)) can be impossible in most real-world scenarios. Rather than trying to answer all the questions about \\(p\\), sometimes it’s still worthwhile to answer “what is the most likely value of \\(p\\) given the data we’ve observed?”\nThis is answered with maximum likelihood estimation, and we need two steps. Using (multivariable) differential calculus, we\n\nfind the value of \\(\\boldsymbol\\theta\\) which maximizes \\(\\mathcal{L}(\\boldsymbol\\theta|\\textbf{x})\\), and\nshow that \\(\\mathcal{L}(\\boldsymbol\\theta|\\textbf{x})\\) is concave down8 at this point.\n\nFor the first step, it is common practice to 1) disregard any multiplicative constants leading \\(\\mathcal{L}\\) (because the derivative in the next step will zero these constants out) and 2) to take the natural logarithm of the likelihood and maximize that instead. Because logarithms simply change the scale of the vertical axis, they do not affect the location of extreme values. Let’s begin (I show what happens to the multiplicative constant in square brackets):\n\\[\\begin{align}\n\\mathcal{L}(p|\\textbf{x}) &= [60\\times]\\ p^2(1 - p)^3,\\ p\\in[0,1] \\\\\n\\Longrightarrow \\qquad \\ell(p|\\textbf{x}) &= [\\log(60) +]\\ 2\\log(p) + 3\\log(1 - p),\\ p\\in(0,1) \\\\\n\\Longrightarrow \\qquad \\frac{\\partial\\ell}{\\partial p} &= [0+]\\ \\frac{2}{p} - \\frac{3}{1 - p} \\\\\n\\Longrightarrow \\qquad 0 &\\overset{\\text{set}}{=} \\frac{2}{p} - \\frac{3}{1 - p} \\\\\n\\Longrightarrow \\qquad 0 &= 2(1 - p) - 3p \\\\\n\\Longrightarrow \\qquad \\hat{p} &= 2/5\n\\end{align}\\]\nThe second step is to confirm that \\(\\hat{p} = \\frac{2}{5}\\) is a maximum of \\(\\mathcal{L}\\), by ensuring that the second derivative of \\(\\mathcal{L}\\) is negative around \\(\\hat{p}\\). For that, we return to the first derivative of the log-likelihood (I’m using negative exponents instead of fractions because the chain rule is easier to apply than the quotient rule for these fractions), and differentiate again:\n\\[\\begin{align}\n\\frac{\\partial\\ell}{\\partial p} &= 2p^{-1} - 3(1 - p)^{-1} \\\\\n\\Longrightarrow \\qquad \\frac{\\partial^2\\ell}{\\partial p^2} &= -2p^{-2} + 3(1 - p)^{-2}\\times(-1) \\\\\n&= -\\left( \\frac{2}{p^2} + \\frac{3}{(1 - p)^2} \\right) &lt;0\\ \\forall p \\in (0,1).\n\\end{align}\\]\nSo, for this trivial example, both Method of Moments and Maximum Likelihood Estimation yielded the same estimate for \\(\\hat{p}\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Bernoulli Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#footnotes",
    "href": "chapters/binomial_20250310.html#footnotes",
    "title": "3  The Binomial Distribution",
    "section": "",
    "text": "Read all the pages of this short lesson, as it also includes a refresher on Riemann-Stieltjes Integrals: https://www.colorado.edu/amath/sites/default/files/attached-files/definetti.pdf↩︎\nThis denotes the factorial of an integer: https://en.wikipedia.org/wiki/Factorial↩︎\nhttps://en.wikipedia.org/wiki/Natural_number↩︎\nhttps://en.wikipedia.org/wiki/Pascal%27s_rule↩︎\nhttps://en.wikipedia.org/wiki/Mathematical_induction↩︎\nhttps://en.wikipedia.org/wiki/Binomial_theorem↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/negative_binomial_20250310.html",
    "href": "chapters/negative_binomial_20250310.html",
    "title": "4  The Negative Binomial Distribution",
    "section": "",
    "text": "4.1 Deriving the Distribution",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Negative Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/negative_binomial_20250310.html#example-random-samples",
    "href": "chapters/negative_binomial_20250310.html#example-random-samples",
    "title": "4  The Negative Binomial Distribution",
    "section": "4.2 Example Random Samples",
    "text": "4.2 Example Random Samples\n\n\nCode\nset.seed(20150516)\n\nN &lt;- 5\n\nxSymm &lt;- rnbinom(n = 500, size = N, prob = 0.5)\nsamplesSymm_ls &lt;- list(\n  n5   = xSymm[1:5],\n  n30  = xSymm[1:30],\n  n100 = xSymm[1:100],\n  n500 = xSymm\n)\nbinsSymm_int &lt;- seq.int(from = -1, to = max(xSymm) + 1, by = 1)\n\nxSkew &lt;- rnbinom(n = 500, size = N, prob = 0.2)\nsamplesSkew_ls &lt;- list(\n  n5   = xSkew[1:5],\n  n30  = xSkew[1:30],\n  n100 = xSkew[1:100],\n  n500 = xSkew\n)\nbinsSkew_int &lt;- seq.int(from = -1, to = max(xSkew) + 1, by = 1)\n# we are drawing until we reach N successes, so the upper limit should be \n# N * (1 / min(prob)) + epsilon\n\nrm(xSymm, xSkew)\n\n\n\n\nCode\npar(mfrow = c(2, 2))\n\nhist(samplesSymm_ls$n5, breaks = binsSymm_int)\nhist(samplesSymm_ls$n30, breaks = binsSymm_int)\nhist(samplesSymm_ls$n100, breaks = binsSymm_int)\nhist(samplesSymm_ls$n500, breaks = binsSymm_int)\n\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(2, 2))\n\nhist(samplesSkew_ls$n5, breaks = binsSkew_int)\nhist(samplesSkew_ls$n30, breaks = binsSkew_int)\nhist(samplesSkew_ls$n100, breaks = binsSkew_int)\nhist(samplesSkew_ls$n500, breaks = binsSkew_int)\n\npar(mfrow = c(1, 1))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Negative Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/negative_binomial_20250310.html#show-that-this-is-a-distribution",
    "href": "chapters/negative_binomial_20250310.html#show-that-this-is-a-distribution",
    "title": "4  The Negative Binomial Distribution",
    "section": "4.3 Show that this is a Distribution",
    "text": "4.3 Show that this is a Distribution",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Negative Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/negative_binomial_20250310.html#derive-the-moment-generating-function",
    "href": "chapters/negative_binomial_20250310.html#derive-the-moment-generating-function",
    "title": "4  The Negative Binomial Distribution",
    "section": "4.4 Derive the Moment Generating Function",
    "text": "4.4 Derive the Moment Generating Function",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Negative Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/negative_binomial_20250310.html#method-of-moments-estimators",
    "href": "chapters/negative_binomial_20250310.html#method-of-moments-estimators",
    "title": "4  The Negative Binomial Distribution",
    "section": "4.5 Method of Moments Estimators",
    "text": "4.5 Method of Moments Estimators",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Negative Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/negative_binomial_20250310.html#maximum-likelihood-estimators",
    "href": "chapters/negative_binomial_20250310.html#maximum-likelihood-estimators",
    "title": "4  The Negative Binomial Distribution",
    "section": "4.6 Maximum Likelihood Estimators",
    "text": "4.6 Maximum Likelihood Estimators",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Negative Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/negative_binomial_20250310.html#exercises",
    "href": "chapters/negative_binomial_20250310.html#exercises",
    "title": "4  The Negative Binomial Distribution",
    "section": "4.7 Exercises",
    "text": "4.7 Exercises",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Negative Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#deriving-the-distribution",
    "href": "chapters/binomial_20250310.html#deriving-the-distribution",
    "title": "3  The Binomial Distribution",
    "section": "",
    "text": "3.1.1 A Primer on Exchangeability\nWhat if we saw \\(\\{\\)Heads, Tails, Tails, Heads, Tails\\(\\}\\) instead? The total number of heads and tails flipped is the same, but the order is different. Does that affect our estimate of \\(p\\)? In order to generalize this experiment a bit, we need to use the principle of exchangeability.1 The general idea of exchangeability is that the order of the observed flips doesn’t matter; i.e., that the coin doesn’t remember flipping a “Head” first then a “Tail”. Recall that we encoded the observed flips as \\(\\textbf{x} = (1,0,1,0,0)\\). Therefore, if the order doesn’t matter, then all these rows will give us the same information about \\(p\\):\n\n# Thanks to ChatGPT for finding this package for me.\ncquad::sq(J = 5, s = 2)\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    0    0    0    1    1\n [2,]    0    0    1    0    1\n [3,]    0    0    1    1    0\n [4,]    0    1    0    0    1\n [5,]    0    1    0    1    0\n [6,]    0    1    1    0    0\n [7,]    1    0    0    0    1\n [8,]    1    0    0    1    0\n [9,]    1    0    1    0    0\n[10,]    1    1    0    0    0\n\nWe see that there are 10 rows here, showing the 10 ways that we could flip 5 coins sequentially and only see 2 heads. While each of these rows shows the outcomes of different events, the resulting probability functions will be the same because of the commutative property of multiplication. That is, the likelihood function for the first row: \\[\n\\prod\\limits_{k = (0, 0, 0, 1, 1)} \\left[ p^{k_i}(1 - p)^{1 - k_i} \\right] = p^{\\sum_k k_i}(1 - p)^{\\sum_k (1 - k_i)} = p^2(1 - p)^3;\n\\] yields the same polynomial as the likelihood function for the last row: \\[\n\\prod\\limits_{k = (1, 1, 0, 0, 0)} \\left[ p^{k_i}(1 - p)^{1 - k_i} \\right] = p^{\\sum_k k_i}(1 - p)^{\\sum_k (1 - k_i)} = p^2(1 - p)^3.\n\\] The likelihood functions will be the same for all 10 rows.\n\n\n3.1.2 The Binomial Coefficient\nLet’s pretend that a prophet tells us that that the next time we flip \\(n = 5\\) coins we will see \\(k = 2\\) heads. We haven’t flipped any coins yet, but we have a vision of the future. If we encode 1 for heads and 0 for tails, we know that what is about to happen when we flip these 5 coins can be described by one of the 10 rows above. But how did we get there?\nTo make this process easier to explain, let’s flip 5 coins and leave them on the table, so that we can “see” our results as they happen. The 10 rows of the matrix above are based on the logic of this process:\n\nBefore I flip any coins, there are \\(n = 5\\) coins in my hand. I haven’t flipped any coins yet, so all my options are available. There are 5 ways to flip one head.\nI flip the first coin, and after I set that coin aside, I have \\(n - 1 = 4\\) coins left for me to flip. There are still 4 ways to flip one head.\nI flip the second coin, and set it aside. I now have \\(n - 2 = 3\\) coins left to flip. There are 3 ways left to flip one head.\nI flip the third coin, and set it aside. Now things get interesting: I have \\(n = k = 2\\) coins left. If I have already flipped two heads in the first three flips, then I know the next two flips must be tails. If I haven’t flipped any heads in the first three flips, then I know the next two flips must be heads. If I’ve only flipped one head in the first three flips, then I know that one of the two next flips will be heads and the other will be tails (but I don’t know which is which).\nI flip the fourth coin and set it aside. The prophet already told me there would only be 2 heads in 5 flips, so the next flip is completely determined. If I have already flipped 2 heads with the first four coins, then this flip MUST be tails. If I’ve only flipped 1 head on the first four coins, then this flip MUST be heads. There is only one possible outcome, and it is predetermined to occur.\n\nIf we hadn’t been told by a prophet ahead of time that we would see two heads, then there would be \\(n\\) options for the first flip (I haven’t decided which coin to pick up yet, so that’s why I have 4 choices), \\(n - 1\\) for the second, all the way down to 1 way to flip at the end. That tells us there are \\(n!\\) ways these flips could have happened. There would have been \\(5\\times 4\\times 3\\times 2 = 120 = n!\\)2 possible orderings and configurations of heads and tails. However, in our process, we’ve already assumed exchangeability, so the order of the flips does not matter. Not only that, but we are further limited: the prophet informed us that we MUST see exactly \\(k = 2\\) heads and \\(n - k = 3\\) tails. Because we use multiplication to “add” new possibilities, we must use division to take away these excluded possibilities. Since we must have \\(k = 2\\) heads, we remove 2 opportunities to flip tails; since we must have \\(n - k = 3\\) tails, we remove 3 opportunities to flip heads. Therefore, the number of options will be: \\[\n\\frac{|\\text{all results}|}{|\\text{removed tails results}| \\times |\\text{removed heads results}|} = \\frac{5!}{2!\\times 3!} = \\frac{120}{2 \\times 6} = 10.\n\\]\n\nWe then define the Binomial Coefficient as \\[\n{n \\choose k} \\equiv \\frac{n!}{k!(n - k)!}.\n\\]\n\n\n3.1.3 Constructing the Distribution\nTo recap, we now have:\n\na statement about the likelihood of a single binary event, of which \\(k = 1\\) denotes one class and \\(k = 0\\) denotes its complement (the other binary class): \\(p^k(1 - p)^{1 - k}\\),\nan experiment that yields a set of \\(n\\) such independent and exchangeable binary events, and\na way to count all the ways that these events could possibly occur: \\(\\frac{n!}{k!(n - k)!}\\).\n\nLet us assume that the number of trials, \\(n\\), is known. We will now construct a probability function for the random variable \\(k = 0, 1, \\ldots, n\\). We still assume independence and exchangeability, so the probability of success, \\(p\\), is fixed. Then, this function will first have a the binomial coefficient, then the probability to observe \\(k\\) successes, and finally the probability to observe \\(n - k\\) failures. Thus, the Binomial Distribution is:\n\\[\nf(k|n,p) \\equiv {n \\choose k} p^{k}(1 - p)^{n - k}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  },
  {
    "objectID": "chapters/binomial_20250310.html#method-of-moments-estimates-from-observed-data",
    "href": "chapters/binomial_20250310.html#method-of-moments-estimates-from-observed-data",
    "title": "3  The Binomial Distribution",
    "section": "3.6 Method of Moments Estimates from Observed Data",
    "text": "3.6 Method of Moments Estimates from Observed Data\nRecall our “observed” data: \\(\\{\\)Heads, Tails, Heads, Tails, Tails\\(\\}\\). This was an observation from 5 independent Bernoulli trials or only ONE Binomial experiment with \\(N=5\\) and \\(k = 2\\). We need more than \\(n = 1\\) if we want to estimate moments! If you remember the introduction, we generated this data from a Bernoulli process with \\(p = 0.35\\). Let’s generate some more samples, this time we will have \\(n = 7\\) experiments where we flipped \\(N = 5\\) coins each time, with the same probability of success \\(p = 0.35\\) as before:\n\n\nCode\n# Reset our seed\nset.seed(20150516)\n\n# Generate our sample\nobserved_int &lt;- rbinom(\n  n = 7,      # number of Binomial experiments\n  size = 5,   # number of Bernoulli Trials per experiment\n  prob = 0.35 # probability of success for each Bernoulli Trial\n)\n\n# Inspect\nobserved_int\n[1] 3 1 2 3 1 0 1\n\n\n\n3.6.1 Case When \\(n\\) is Known\nThis is the most common case. Recall that we need the averages of \\(k_i\\) and \\(k_i^2\\), so let’s calculate these first.\n\n\nCode\nk_df &lt;- \n  tibble(k = observed_int) %&gt;% \n  mutate(k2 = k^2)\nk_df\n# A tibble: 7 × 2\n      k    k2\n  &lt;int&gt; &lt;dbl&gt;\n1     3     9\n2     1     1\n3     2     4\n4     3     9\n5     1     1\n6     0     0\n7     1     1\n\ncolSums(k)\nError: object 'k' not found\n\n\nSo, \\(\\sum_i k_i = 2\\) and \\(\\sum_i k_i^2 = 2\\). Thus, the two equations in our system are\n\n\\(np = 2\\)\n\\(np(np - p + 1) = 2\\)\n\nHowever, we already said that \\(n=5\\) is known, so we don’t need the second equation. All we have to solve is \\(np = (5)p = 2 \\Rightarrow \\hat{p}_{MoM} = 0.4\\).\n\n3.6.1.1 Case When \\(n\\) is Unknown\nThis is a rare case, and usually only a theoretical exercise, though it can happen in ecological studies of species. One example could be where a park ranger goes deep into the woods on 5 different days. While on patrol, they encounter one grey wolf on the first day, none on the second, one on the third, and none on the fourth and fifth. In this case, we don’t actually know how many grey wolves are in the park close enough to the ranger to even be detected. Thus, we could think about this as 5 Binomial trials with unknown \\(n\\).\nNow, we have the same system as above, but it is no longer trivial. So, because by the first equation we have that \\(np = 2\\), \\[\n\\begin{aligned}\n2 &= np(np - p + 1) \\\\\n\\Longrightarrow 2 &= (2)([2]) - p + 1) \\\\\n\\Longrightarrow 1 &= 3 - p \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Binomial Distribution</span>"
    ]
  }
]