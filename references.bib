@article{knuth84,
  author = {Knuth, Donald E.},
  title = {Literate Programming},
  year = {1984},
  issue_date = {May 1984},
  publisher = {Oxford University Press, Inc.},
  address = {USA},
  volume = {27},
  number = {2},
  issn = {0010-4620},
  url = {https://doi.org/10.1093/comjnl/27.2.97},
  doi = {10.1093/comjnl/27.2.97},
  journal = {Comput. J.},
  month = may,
  pages = {97–111},
  numpages = {15}
}


@article{xue_regression_1997,
	title = {Regression analysis of discrete time survival data under heterogeneity},
	volume = {16},
	copyright = {Copyright © 1997 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0258%2819970915%2916%3A17%3C1983%3A%3AAID-SIM628%3E3.0.CO%3B2-3},
	doi = {10.1002/(SICI)1097-0258(19970915)16:17<1983::AID-SIM628>3.0.CO;2-3},
	abstract = {This paper concerns the regression analysis of discrete time survival data for heterogeneous populations by means of frailty models. We express the survival time for each individual as a sequence of binary variables that indicate if the individual survived at each time point. The main result is that the likelihood for these indicators can be factored into contributions that involve the conditional survival probabilities integrated over the frailty distribution of the risk set (population-averaged). We then model these population-averaged conditional probabilities as a function of covariates. The result justifies the practice of treating the failure indicators as independent Bernoulli trials and fitting binary regression models for the conditional failure probabilities at each time point. However, we must interpret the regression coefficients as population-averaged rather than subject-specific parameters. We apply the method to the Framingham Heart Study on risk factors for cardiovascular disease. © 1997 by John Wiley \& Sons, Ltd.},
	language = {en},
	number = {17},
	urldate = {2025-09-11},
	journal = {Statistics in Medicine},
	author = {Xue, Xiaonan and Brookmeyer, Ron},
	year = {1997},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-0258\%2819970915\%2916\%3A17\%3C1983\%3A\%3AAID-SIM628\%3E3.0.CO\%3B2-3},
	pages = {1983--1993},
	file = {Full Text PDF:C\:\\Users\\samagonz\\Zotero\\storage\\XWUEMP4P\\xue and brookmeyer1997_regression_analysis_of_discrete_time_survival_data_under_heterogeneity.pdf:application/pdf;Snapshot:C\:\\Users\\samagonz\\Zotero\\storage\\74N6RJK7\\(SICI)1097-0258(19970915)16171983AID-SIM6283.0.html:text/html},
}

@inproceedings{bemando_machine-learning-based_2021,
	title = {Machine-{Learning}-{Based} {Prediction} {Models} of {Coronary} {Heart} {Disease} {Using} {Naïve} {Bayes} and {Random} {Forest} {Algorithms}},
	url = {https://ieeexplore.ieee.org/abstract/document/9537060},
	doi = {10.1109/ICSECS52883.2021.00049},
	abstract = {Coronary heart disease (CHD), alternatively known as cardiovascular disease (CVD) is the number one cause of death in the world. Accordingly, a plethora of research have been conducted to predict the early diagnosis of the heart disease and determine the most important risk factors associated with the disease. Despite these considerable efforts, the accuracy of the prediction has remained inadequate and the most important risk factors have remained elusive. This research paper discusses many risk factors associated with the disease and presents the prediction models of coronary heart disease using supervised machine learning algorithms, namely Gaussian Naïve Bayes, Bernoulli Naïve Bayes and Random Forest algorithms. It uses the public dataset from the Cleveland database of UCI repository of coronary heart disease patients. The results show that the Gaussian Naïve Bayes, Bernoulli Naïve Bayes and Random Forest algorithms have accuracies of 85.00\%, 85.00\% and 75.00\%, respectively. Moreover, the precision, F-measure and recall of the Gaussian and Bernoulli Naïve Bayes are higher than those of Random Forest algorithm, signifying its importance in predicting the early diagnosis of the disease.},
	urldate = {2025-09-11},
	booktitle = {2021 {International} {Conference} on {Software} {Engineering} \& {Computer} {Systems} and 4th {International} {Conference} on {Computational} {Science} and {Information} {Management} ({ICSECS}-{ICOCSIM})},
	author = {Bemando, Charles and Miranda, Eka and Aryuni, Mediana},
	month = aug,
	year = {2021},
	keywords = {Bernoulli Naïve Bayes, Computational modeling, Databases, factors, Gaussian Naïve Bayes, Heart, heart disease, machine learning, Machine learning algorithms, Prediction algorithms, Predictive models, Random Forest, risk, Scientific computing},
	pages = {232--237},
	file = {Full Text PDF:C\:\\Users\\samagonz\\Zotero\\storage\\KAY9EU3N\\bemando2021_machine-learning-based_prediction_models_of_coronary_heart_disease_using_naïve_bayes_and_random_fore.pdf:application/pdf},
}

@misc{thompson_minimum_2010,
	title = {Minimum {Variance} {Estimators}},
	url = {https://faculty.washington.edu/eathomp/S341_10/Hwks/Cramer_Rao_soln.pdf},
	urldate = {2025-09-14},
	author = {Thompson, E.A.},
	month = feb,
	year = {2010},
}

@book{mccullagh_generalized_2018,
	address = {Boca Raton},
	title = {Generalized {Linear} {Models}},
	isbn = {978-0-412-31760-6},
	abstract = {The success of the first edition of Generalized Linear Models led to the updated Second Edition, which continues to provide a definitive unified, treatment of methods for the analysis of diverse types of data. Today, it remains popular for its clarity, richness of content and direct relevance to agricultural, biological, health, engineering, and other applications.The authors focus on examining the way a response variable depends on a combination of explanatory variables, treatment, and classification variables. They give particular emphasis to the important case where the dependence occurs through some unknown, linear combination of the explanatory variables.The Second Edition includes topics added to the core of the first edition, including conditional and marginal likelihood methods, estimating equations, and models for dispersion effects and components of dispersion. The discussion of other topics-log-linear and related models, log odds-ratio regression models, multinomial response models, inverse linear and related models, quasi-likelihood functions, and model checking-was expanded and incorporates significant revisions.Comprehension of the material requires simply a knowledge of matrix theory and the basic ideas of probability theory, but for the most part, the book is self-contained. Therefore, with its worked examples, plentiful exercises, and topics of direct use to researchers in many disciplines, Generalized Linear Models serves as ideal text, self-study guide, and reference.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {McCullagh, P. and Nelder, John A.},
	year = {2018},
}

@book{gelman_bayesian_2013,
	edition = {3},
	title = {Bayesian {Data} {Analysis}},
	publisher = {Chapman \& Hall/CRC},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
	year = {2013},
}

@book{hosmer_applied_2013,
	edition = {3},
	title = {Applied {Logistic} {Regression}},
	publisher = {Wiley},
	author = {Hosmer, David W. and Lemeshow, Stanley and Sturdivant, Rodney X.},
	month = mar,
	year = {2013},
}

@book{johnson_univariate_2005,
	edition = {3},
	title = {Univariate {Discrete} {Distributions}},
	isbn = {978-0-471-27246-5},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/0471715816},
	publisher = {Wiley},
	author = {Johnson, Norman L. and Kemp, Adrienne W. and Kotz, Samuel},
	month = jan,
	year = {2005},
}

@book{casella_statistical_2002,
	edition = {2},
	title = {Statistical {Inference}},
	publisher = {Duxbury Advanced Series},
	author = {Casella, George and Berger, L., Roger},
	year = {2002},
}

@article{agresti_approximate_1998,
	title = {Approximate is {Better} than “{Exact}” for {Interval} {Estimation} of {Binomial} {Proportions}},
	volume = {52},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.1998.10480550},
	doi = {10.1080/00031305.1998.10480550},
	abstract = {For interval estimation of a proportion, coverage probabilities tend to be too large for “exact” confidence intervals based on inverting the binomial test and too small for the interval based on inverting the Wald large-sample normal test (i.e., sample proportion ± z-score × estimated standard error). Wilson's suggestion of inverting the related score test with null rather than estimated standard error yields coverage probabilities close to nominal confidence levels, even for very small sample sizes. The 95\% score interval has similar behavior as the adjusted Wald interval obtained after adding two “successes” and two “failures” to the sample. In elementary courses, with the score and adjusted Wald methods it is unnecessary to provide students with awkward sample size guidelines.},
	number = {2},
	urldate = {2025-09-14},
	journal = {The American Statistician},
	author = {Agresti, Alan and Coull, Brent A.},
	month = may,
	year = {1998},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/00031305.1998.10480550},
	keywords = {Score test, Confidence interval, Discrete distribution, Exact inference, Poisson distribution, Small sample},
	pages = {119--126},
}



