---
title: "The Binomial Distribution"
author: "Gabriel J. Odom"
date: last-modified
format:
  html:
    embed-resources: true
    code-fold: true
knitr:
  opts_chunk:        ########## set global options ############
    collapse: true   # keep code from blocks together (if shown)
    echo: true       # show code
    message: true    # show messages
    warning: true    # show warnings
    error: true      # show error messages
    comment: ""      # don't show ## with printed output
    dpi: 100         # image resolution (typically 300 for publication)
    fig-width: 6.5   # figure width
    fig-height: 4.0  # figure height
    R.options:    
      digits: 3    # round to three digits
editor: source
---

```{r, tidyverse-tidymodels}
#| echo: false

library(conflicted)
suppressPackageStartupMessages(library(tidymodels))
tidymodels_prefer()
suppressPackageStartupMessages(library(tidyverse))

# suppress "`summarise()` has grouped output by " messages
options(dplyr.summarise.inform = FALSE)
```



## Deriving the Distribution
In the previous chapter, we explored the properties of a Bernoulli trial. We envisioned a scenario where a person flipped a coin five times, and the results were $\{$Heads, Tails, Heads, Tails, Tails$\}$. What we knew, but the hypothetical person did no know, was that this particular coin was not "fair". In fact, these five observations were drawn from a Bernoulli process with $P[\text{Heads}] = 0.35$.


### A Primer on Exchangeability
What if we saw $\{$Heads, Tails, Tails, Heads, Tails$\}$ instead? The total number of heads and tails flipped is the same, but the order is different. Does that affect our estimate of $p$? In order to generalize this experiment a bit, we need to use the principle of **exchangeability**.^[Read all the pages of this short lesson, as it also includes a refresher on Riemann-Stieltjes Integrals: <https://www.colorado.edu/amath/sites/default/files/attached-files/definetti.pdf>] The general idea of exchangeability is that the order of the observed flips doesn't matter; i.e., that the coin doesn't remember flipping a "Head" first then a "Tail". Recall that we encoded the observed flips as $\textbf{x} = (1,0,1,0,0)$. Therefore, if the order doesn't matter, then all these rows will give us the same information about $p$:
```{r}
#| label: all-binom-trials-yielding-2-wins-in-5-flips
#| code-fold: false

# Thanks to ChatGPT for finding this package for me.
cquad::sq(J = 5, s = 2)
```

We see that there are 10 rows here, showing the 10 ways that we could flip 5 coins sequentially and only see 2 heads. While each of these rows shows the outcomes of different events, the resulting probability functions will be the same because of the **commutative property** of multiplication. That is, the likelihood function for the first row:
$$
\prod\limits_{k = (0, 0, 0, 1, 1)} \left[ p^{k_i}(1 - p)^{1 - k_i} \right] = p^{\sum_k k_i}(1 - p)^{\sum_k (1 - k_i)} = p^2(1 - p)^3;
$$
yields the same polynomial as the likelihood function for the last row:
$$
\prod\limits_{k = (1, 1, 0, 0, 0)} \left[ p^{k_i}(1 - p)^{1 - k_i} \right] = p^{\sum_k k_i}(1 - p)^{\sum_k (1 - k_i)} = p^2(1 - p)^3.
$$
The likelihood functions will be the same for all 10 rows.


### The Binomial Coefficient
Let's pretend that a prophet tells us that that the next time we flip $n = 5$ coins we will see $k = 2$ heads. We haven't flipped any coins yet, but we have a vision of the future. If we encode 1 for heads and 0 for tails, we know that what is about to happen when we flip these 5 coins can be described by one of the 10 rows above. But how did we get there?

To make this process easier to explain, let's flip 5 coins and leave them on the table, so that we can "see" our results as they happen. The 10 rows of the matrix above are based on the logic of this process:

1. Before I flip any coins, there are $n = 5$ coins in my hand. I haven't flipped any coins yet, so all my options are available. There are 5 ways to flip one head.
2. I flip the first coin, and after I set that coin aside, I have $n - 1 = 4$ coins left for me to flip. There are still 4 ways to flip one head.
3. I flip the second coin, and set it aside. I now have $n - 2 = 3$ coins left to flip. There are 3 ways left to flip one head.
4. I flip the third coin, and set it aside. Now things get interesting: I have $n = k = 2$ coins left. If I have already flipped two heads in the first three flips, then I know the next two flips must be tails. If I haven't flipped any heads in the first three flips, then I know the next two flips must be heads. If I've only flipped one head in the first three flips, then I know that one of the two next flips will be heads and the other will be tails (but I don't know which is which).
5. I flip the fourth coin and set it aside. The prophet already told me there would only be 2 heads in 5 flips, so the next flip is completely determined. If I have already flipped 2 heads with the first four coins, then this flip MUST be tails. If I've only flipped 1 head on the first four coins, then this flip MUST be heads. There is only one possible outcome, and it is predetermined to occur.

If we hadn't been told by a prophet ahead of time that we would see two heads, then there would be $n$ options for the first flip (I haven't decided which coin to pick up yet, so that's why I have 4 choices), $n - 1$ for the second, all the way down to 1 way to flip at the end. That tells us there are $n!$ ways these flips could have happened. There would have been $5\times 4\times 3\times 2 = 120 = n!$^[This denotes the **factorial** of an integer: <https://en.wikipedia.org/wiki/Factorial>] possible orderings and configurations of heads and tails. However, in our process, we've already assumed exchangeability, so the order of the flips does not matter. Not only that, but we are further limited: the prophet informed us that we MUST see exactly $k = 2$ heads and $n - k = 3$ tails. *Because we use multiplication to "add" new possibilities, we must use division to take away these excluded possibilities.* Since we must have $k = 2$ heads, we remove 2 opportunities to flip tails; since we must have $n - k = 3$ tails, we remove 3 opportunities to flip heads. Therefore, the number of options will be:
$$
\frac{|\text{all results}|}{|\text{removed tails results}| \times |\text{removed heads results}|} = \frac{5!}{2!\times 3!} = \frac{120}{2 \times 6} = 10.
$$

<!-- There are n*(n-1)...(k+1)*k! ways to flip everything, and k! ways to get the heads and (n - k)! ways to get the tails. -->

We then define the **Binomial Coefficient** as
$$
{n \choose k} \equiv \frac{n!}{k!(n - k)!}.
$$


### Constructing the Distribution
To recap, we now have:

1. a statement about the likelihood of a single binary event, of which $k = 1$ denotes one class and $k = 0$ denotes its complement (the other binary class): $p^k(1 - p)^{1 - k}$,
2. an experiment that yields a set of $n$ such independent and exchangeable binary events, and
3. a way to count all the ways that these events could possibly occur: $\frac{n!}{k!(n - k)!}$.

Let us assume that the number of trials, $n$, is known. We will now construct a probability function for the random variable $k = 0, 1, \ldots, n$. We still assume independence and exchangeability, so the probability of success, $p$, is fixed. Then, this function will first have a the binomial coefficient, then the probability to observe $k$ successes, and finally the probability to observe $n - k$ failures. Thus, the **Binomial Distribution** is:

$$
f(k|n,p) \equiv {n \choose k} p^{k}(1 - p)^{n - k}.
$$

</br>



## Example Random Samples

```{r, random-sample}
set.seed(20150516)

N <- 10
bins_int <- seq.int(from = -1, to = N, by = 1)

xSymm <- rbinom(n = 100, size = N, prob = 0.5)
samplesSymm_ls <- list(
  n5   = xSymm[1:5],
  n15  = xSymm[1:15],
  n30  = xSymm[1:30],
  n100 = xSymm
)

xSkew <- rbinom(n = 100, size = N, prob = 0.2)
samplesSkew_ls <- list(
  n5   = xSkew[1:5],
  n15  = xSkew[1:15],
  n30  = xSkew[1:30],
  n100 = xSkew
)

rm(xSymm, xSkew)
```

```{r}
#| label: random-sample-hist-symm
#| fig-show: "hold"

par(mfrow = c(2, 2))

hist(samplesSymm_ls$n5, breaks = bins_int)
hist(samplesSymm_ls$n15, breaks = bins_int)
hist(samplesSymm_ls$n30, breaks = bins_int)
hist(samplesSymm_ls$n100, breaks = bins_int)

par(mfrow = c(1, 1))
```

```{r}
#| label: random-sample-hist-skew
#| fig-show: "hold"

par(mfrow = c(2, 2))

hist(samplesSkew_ls$n5, breaks = bins_int)
hist(samplesSkew_ls$n15, breaks = bins_int)
hist(samplesSkew_ls$n30, breaks = bins_int)
hist(samplesSkew_ls$n100, breaks = bins_int)

par(mfrow = c(1, 1))
```


</br>



## Show that this is a Distribution
Let $\mathcal{S} = \mathbb{N} \cup 0$, where $\mathbb{N}$ denotes the set of **natural numbers**.^[<https://en.wikipedia.org/wiki/Natural_number>] Let $f(k|n,p) = {n \choose k} p^{k}(1 - p)^{n - k}$ represent the probability function of the Binomial Distribution. We must now show that

1. $\forall k \in \mathcal{S}$, and for $p \in (0,1)$, $f(k|n, p) \ge 0$, and
2. for $p \in (0,1)$, $\int_{k \in \mathcal{S}} \text{d}F(k|n,p) = 1$.


### Formal Foundations: Pascal's Rule
For writing proofs involving combinations, we need the following property, known as **Pascal's Formula**,^[<https://en.wikipedia.org/wiki/Pascal%27s_rule>] which states that

$$
{a \choose b} = {a - 1 \choose b} + {a - 1 \choose b - 1}.
$$

*Proof:* For integers $a,b$, we construct this identity directly via simplification:
$$
\begin{aligned}
{a - 1 \choose b} + {a - 1 \choose b - 1} &=
  \frac{(a - 1)!}{b!(a - b - 1)!} + \frac{(a - 1)!}{(b - 1)!(a - 1 - b + 1)!} \\
  &= \frac{a - b}{a - b}\left[ \frac{(a - 1)!}{b!(a - b - 1)!} \right] + \frac{b}{b} \left[ \frac{(a - 1)!}{(b - 1)!(a - b)!} \right] \\
  &= \frac{a(a - 1)! - b(a - 1)!}{b!(a - b)(a - b - 1)!} + \frac{b(a - 1)!}{b(b - 1)!(a - b)!} \\
  &= \frac{a! - b(a - 1)!}{b!(a - b)!} + \frac{b(a - 1)!}{b!(a - b)!} \\
  &= \frac{a!}{b!(a - b)!} \\
  &\equiv {a \choose b}.
\end{aligned}
$$


### Formal Foundations: Mathematical Induction 
The next foundational proof requires a technique called "Proof by Induction", or more generally, **mathematical induction**.^[<https://en.wikipedia.org/wiki/Mathematical_induction>] This is the structure of such a proof:

> **Proof by Induction**: Consider a sequence of equations indexed over the integers by $n$. To show that the sequence of equations is true $\forall n$, we
>
1) show that the equation is true for $n = 1$ [the "base case"],
2) assume that the equation is true for $n = i$ [the "hypothesis"], then
3) prove that the equation is true for $n = i + 1$ from the case when $n = i$ [the "induction"].

Here is a trivial example. Let's prove $\forall k \in\mathbb{N} \ge 5$ that $2^k < \Gamma(k + 1)$ (the point of this proof is to show that the factorial function increases more rapidly to $\infty$ than the exponential function). Our **base case** is for $k = 5$. We know that $2^5 = 32$ and $\Gamma(k + 1) = k! = 5! = 120$. Because $32 < 120$, the base case is true. Our **hypothesis**, what we assume to be true, is that $2^i < \Gamma(i + 1)$. To logically **induct**, we assume our hypothesis is true, and then show that our hypothesis implies that $2^{i + 1} < \Gamma(i + 2)$. That is
$$
\begin{aligned}
2^{i+1} &\overset{?}{<} \Gamma(i + 2) \\
\Longrightarrow 2 \times 2^i &\overset{?}{<} i \times \Gamma(i + 1) \\
\Longrightarrow \frac{2}{i} \times 2^i &< \Gamma(i + 1),
\end{aligned}
$$
which is true for $i \ge 5$ because our hypothesis was that $2^i < \Gamma(i + 1)$. Thus, $2^k < \Gamma(k + 1)\ \forall k \in\mathbb{N} \ge 5$, which completes our proof.


### Formal Foundations: Mathematical Induction Proof of the Binomial Theorem
For this section, we will also need to use the **Binomial Theorem**:^[<https://en.wikipedia.org/wiki/Binomial_theorem>] 

$$
(x +  y)^n = \sum_{k = 0}^n {n \choose k} x^k y^{n - k}.
$$

Before we can use this, we must prove that it is true. 

#### The Base Case
Let $n = 1$. Then

$$
\begin{aligned}
\sum_{k = 0}^1 {1 \choose k} x^k y^{1 - k} &=
  \left[ {1 \choose 0} x^0 y^{1 - 0} \right] + \left[ {1 \choose 1} x^1 y^{1 - 1} \right] \\
  &= \frac{1!}{0!\times 1!} (1) y^1 + \frac{1!}{1!\times 0!} x^1(1) \\
  &= y + x \\
  &= (x + y)^1.
\end{aligned}
$$

#### The Hypothesis
We assume that this equation is true for $n = i$. That is, we assume that

$$
\sum_{k = 0}^i {i \choose k} x^k y^{i - k} = (x + y)^i.
$$

#### The Induction
Assuming that the hypothesis for $n = i$ is true, we will show that the equation also holds for $n = i + 1$. Note that the end of the proof requires Pascal's Rule to combine sums of combinations, and we comment that there is only one way to "choose" 0 things or all things. Thus,

$$
\begin{aligned}
(x + y)^i &= \sum_{k = 0}^i {i \choose k} x^k y^{i - k} \\
\Longrightarrow (x + y)^{i + 1} &= (x + y)\sum_{k = 0}^i {i \choose k} x^k y^{i - k} \\
  &= (x + y)\left[ {i \choose 0} x^0 y^{i - 0} + {i \choose 1} x^1 y^{i - 1} + \ldots + {i \choose i - 1} x^{i - 1} y^1 + {i \choose i} x^{i - 0} y^0 \right] \\
  &= x\left[ {i \choose 0} x^0 y^i + {i \choose 1} x^1 y^{i - 1} + \ldots + {i \choose i - 1} x^{i - 1} y^1 + {i \choose i} x^i y^0 \right] + \\
  &\quad\ \  y\left[ {i \choose 0} x^0 y^i + {i \choose 1} x^1 y^{i - 1} + \ldots + {i \choose i - 1} x^{i - 1} y^1 + {i \choose i} x^i y^0 \right] \\
  &= \left[ {i \choose 0} x^1 y^i + {i \choose 1} x^2 y^{i - 1} + \ldots + {i \choose i - 1} x^i y^1 + {i \choose i} x^{i+1} y^0 \right] + \\
  &\quad\  \left[ {i \choose 0} x^0 y^{i + 1} + {i \choose 1} x^1 y^i + \ldots + {i \choose i - 1} x^{i - 1} y^2 + {i \choose i} x^i y^1 \right] \\
  \\[0.1mm]
  &\qquad \text{\emph{Collect like terms...}} \\
  &= {i \choose 0} x^0 y^{i + 1} + \\
  &\qquad \left[ {i \choose 0} x^1 y^i + {i \choose 1} x^1 y^i\right] + \ldots + \left[ {i \choose i - 1} x^i y^1 + {i \choose i} x^i y^1 \right] + \\
  &\qquad {i \choose i} x^{i+1} y^0 \\
  \\[0.1mm]
  &\qquad \text{\emph{Only one way to choose all or none...}} \\
  &= (1) x^0 y^{i + 1} + \\
  &\qquad \left[ {i \choose 0} + {i \choose 1} \right]x^1 y^i + \ldots + \left[ {i \choose i - 1} + {i \choose i} \right]x^i y^1 + \\
  &\qquad (1) x^{i+1} y^0 \\
  \\[0.1mm]
  &\qquad \text{\emph{Pascal's Rule...}} \\
  &= (1) x^0 y^{i + 1} + \\
  &\qquad \left[ {i + 1 \choose 1} \right]x^1 y^i + \ldots + \left[ {i + 1 \choose i} \right]x^i y^1 + \\
  &\qquad (1) x^{i+1} y^0 \\
  &= {i + 1 \choose 0} x^0 y^{i + 1} + {i + 1 \choose 1}x^1 y^i + \ldots + {i + 1 \choose i}x^i y^1 + {i + 1 \choose i + 1} x^{i+1} y^0 \\
  \\[0.1mm]
  &\qquad \text{\emph{By definition...}} \\
  &= \sum_{k = 0}^{i + 1} {i + 1 \choose k} x^k y^{i + 1 - k}.
\end{aligned}
$$

Therefore, $\forall n \in \mathbb{N}$,
$$
\sum_{k = 0}^n {n \choose k} x^k y^{n - k} = (x + y)^n.
$$


### The Distribution is Non-negative
Consider $f$ defined above. First, we notice that the Binomial Coefficient is defined as a ratio of factorials; the standard definition of factorials only includes the natural numbers ($\mathbb{N}$), so they are necessarily positive. The ratio of two positive numbers is positive. Second, we have that $k,\ n - k \ge 0$, and that $p > 0$. Non-negative powers of positive numbers are also positive. Setting $p = 0$ yields a degenerate distribution anyway, so we don't bother with it. Putting these together for $p \in (0,1)$, we have that $f = 0$ outside the support of $k$ and $f > 0$ for $k \le n, \ni \{k,\ n\} \in \mathcal{S}$.


### The Total Probability is 1
Recall the **Binomial Theorem** we proved above, and let $x = p$ and $y = 1 - p$. Then,
$$
\begin{aligned}
\int_{k \in \mathcal{S}} \text{d}F(k|n,p) &= \sum_{k = 0}^n {n \choose k} p^{k}(1 - p)^{n - k} \\
  &= \left[ p + (1 - p) \right]^n \\
  &= 1^n \\
  &= 1.
\end{aligned}
$$

Therefore, because the function $f(k|n,p)$ is always non-negative and it's Riemann-Stieljes integral is 1, the Binomial Distribution is a true distribution.

</br>



## Derive the Moment Generating Function
Deriving the MGF for the Binomial is a straightforward application of the definition and Binomial Theorem. That is,
$$
\begin{aligned}
M_k(t) &\equiv \mathbb{E}\left[ e^{tX} \right] \\
  &= \int\limits_{k \in \mathcal{S}(K)} e^{tk} \text{d}F_K(k|n,p) \\
  &= \sum\limits_{k = 0}^n e^{tk} {n \choose k} p^{k}(1 - p)^{n - k} \\
  &= \sum\limits_{k = 0}^n {n \choose k} \left[e^t\right]^k p^{k}(1 - p)^{n - k} \\
  &= \sum\limits_{k = 0}^n {n \choose k} \left[e^t\right]^k p^{k}(1 - p)^{n - k} \\
  &= \sum\limits_{k = 0}^n {n \choose k} \left[e^tp\right]^k (1 - p)^{n - k} \\
  &= \left[ e^tp + (1 - p) \right]^n.
\end{aligned}
$$

</br>



## Method of Moments Estimators
Now that we have this MGF, we can find the Method of Moments (MoM) estimator for $p$, and we will comment on such an estimator for $n$ when it is unknown.


### The First Moment
We have that
$$
\begin{aligned}
M_k(t) &= \left[ e^tp + (1 - p) \right]^n \\
\Longrightarrow M^{\prime}_k(t) &= \frac{\partial}{\partial t} \left[ e^tp + (1 - p) \right]^n \\
  &= n \left[ e^tp + (1 - p) \right]^{n - 1} \frac{\partial}{\partial t} \left[ e^tp + (1 - p) \right] \\
  &= n \left[ e^tp + (1 - p) \right]^{n - 1} e^tp \\
\Longrightarrow M^{\prime}_k(0) &= n \left[ (1)p + (1 - p) \right]^{n - 1} (1)p \\
  &= n(1)^{n - 1}p \\
  &= np.
\end{aligned}
$$
Therefore, $\mathbb{E}[k] = np$.


### The Second Non-Central Moment
Taking the second derivative with respect to $t$, we have
$$
\begin{aligned}
M^{\prime}_k(t) &= ne^tp \left[ e^tp + (1 - p) \right]^{n - 1} \\
\Longrightarrow M^{\prime\prime}_k(t) &= ne^tp \frac{\partial}{\partial t} \left[ e^tp + (1 - p) \right]^{n - 1} + \left[ e^tp + (1 - p) \right]^{n - 1} \frac{\partial}{\partial t} ne^tp \\
  &= ne^tp \times (n - 1) \left[ e^tp + (1 - p) \right]^{n - 2} e^tp + \left[ e^tp + (1 - p) \right]^{n - 1} ne^tp \\
\Longrightarrow M^{\prime\prime}_k(0) &= n(1)p \times (n - 1) \left[ (1)p + (1 - p) \right]^{n - 2} (1)p + \left[ (1)p + (1 - p) \right]^{n - 1} n(1)p \\
  &= np^2 (n - 1) (1)^{n - 2} + np (1)^{n - 1} \\
  &= np \left[(n - 1)p + 1\right].
\end{aligned}
$$
Therefore, $\mathbb{E}[k^2] = np(np - p + 1)$.


### The Second Central Moment
Thus
$$
\begin{aligned}
\text{Var}[k] &= \mathbb{E}[k^2] - \left(\mathbb{E}[k]\right)^2 \\
  &= np(np - p + 1) - \left( np \right)^2 \\
  &= np \left[ np - p + 1 - np \right] \\
  &= np (1 - p).
\end{aligned}
$$

> Technically, we don't need the second central moment to find the Method of Moments estimators, but it is good practice.


### Solving the System of Equations
Now that we have the first two population moments, $\mathbb{E}[k]$ and $\text{Var}[k]$, we can set them equal to the first two sample moments. That is, we solve the system
$$
\left\{ \mathbb{E}[k] = \frac{1}{N} \sum_{i = 1}^N k_i;\ \mathbb{E}[k^2] = \frac{1}{N} \sum_{i = 1}^N k_i^2 \right\},
$$
where $N$ is the number of Bernoulli trials in each Binomial experiment and $k_i$ is the number of successes out of $N$ attempts in Binomial experiment $i$. For our example data, we were discussing the first such experiment, with observed data $\{$Heads, Tails, Heads, Tails, Tails$\}$; so we are on Binomial experiment $i$, the number of coin flips was $N = 5$, and we observed $k_i = 2$ heads.

Given repeated $n$ Binomial experiments of $N$ Bernoulli trials each, we then solve the following system of equations:
$$
\left\{ np = \frac{1}{N} \sum_{i = 1}^N k_i;\ np(np - p + 1) = \frac{1}{N} \sum_{i = 1}^N k_i^2 \right\}.
$$
Very often, $n$ is known, so this simplifies to solving $\hat{p}_{MoM} = \frac{1}{nN} \sum_{i = 1}^N k_i$.

</br>



## Method of Moments Estimates from Observed Data
Recall our "observed" data: $\{$Heads, Tails, Heads, Tails, Tails$\}$. This was an observation from 5 independent Bernoulli trials or only **ONE** Binomial experiment with $N=5$ and $k = 2$. We need more than $n = 1$ if we want to estimate moments! If you remember the introduction, we generated this data from a Bernoulli process with $p = 0.35$. Let's generate some more samples, this time we will have $n = 7$ experiments where we flipped $N = 5$ coins each time, with the same probability of success $p = 0.35$ as before:
```{r}
#| label: generate-more-binomial-data

# Reset our seed
set.seed(20150516)

# Generate our sample
observed_int <- rbinom(
  n = 7,      # number of Binomial experiments
  size = 5,   # number of Bernoulli Trials per experiment
  prob = 0.35 # probability of success for each Bernoulli Trial
)

# Inspect
observed_int
```


### Case When $n$ is Known
This is the most common case. Recall that we need the averages of $k_i$ and $k_i^2$, so let's calculate these first.
```{r}
#| label: k_i-and-k_i-squared

k_df <- 
  tibble(k = observed_int) %>% 
  mutate(k2 = k^2)
k_df

colSums(k)
```



So, $\sum_i k_i = 2$ and $\sum_i k_i^2 = 2$. Thus, the two equations in our system are

1. $np = 2$
2. $np(np - p + 1) = 2$

*However*, we already said that $n=5$ is known, so we don't need the second equation. All we have to solve is $np = (5)p = 2 \Rightarrow \hat{p}_{MoM} = 0.4$.


#### Case When $n$ is Unknown
This is a rare case, and usually only a theoretical exercise, though it can happen in ecological studies of species. One example could be where a park ranger goes deep into the woods on 5 different days. While on patrol, they encounter one grey wolf on the first day, none on the second, one on the third, and none on the fourth and fifth. In this case, we don't actually know how many grey wolves are in the park close enough to the ranger to even be detected. Thus, we could think about this as 5 Binomial trials with unknown $n$.

Now, we have the same system as above, but it is no longer trivial. So, because by the first equation we have that $np = 2$, 
$$
\begin{aligned}
2 &= np(np - p + 1) \\
\Longrightarrow 2 &= (2)([2]) - p + 1) \\
\Longrightarrow 1 &= 3 - p \\
\end{aligned}
$$


</br>



## Maximum Likelihood Estimators


</br>



## Exercises


